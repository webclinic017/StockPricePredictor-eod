{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "excel_reports = \".\\Excel reports\"\n",
    "saved_models = \".\\Saved models\"\n",
    "trained_models = \".\\Trained models\"\n",
    "ticker = 'nvda'\n",
    "\n",
    "#Delete folders\n",
    "# shutil.rmtree(excel_reports)\n",
    "# shutil.rmtree(saved_models)\n",
    "# shutil.rmtree(trained_models)\n",
    "\n",
    "#Create folder, if exist pass exception\n",
    "try:\n",
    "    os.mkdir(\"Excel reports\")\n",
    "    os.mkdir(\"Saved models\")\n",
    "    os.mkdir(\"Trained models\")\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------> PullData completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import PullData\n",
    "\n",
    "#Initiate parameters\n",
    "window_size = 25\n",
    "formation_window = 24\n",
    "target_window = 1\n",
    "\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2022-08-17'\n",
    "\n",
    "GetData = PullData()\n",
    "\n",
    "GetData.fit(ticker=ticker,\n",
    "            start_date=start_date, \n",
    "            end_date=end_date, \n",
    "            interval='1wk',\n",
    "            progress=False,\n",
    "            condition=False,\n",
    "            form_window=formation_window,\n",
    "            target_window=target_window,\n",
    "            timeperiod1=6,\n",
    "            timeperiod2=12,\n",
    "            timeperiod3=24,\n",
    "            export_excel=True,\n",
    "            excel_path = excel_reports\n",
    "            )\n",
    "\n",
    "data_prep = GetData.transform()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape:  (2825, 9)\n",
      "Number of formations:  113\n",
      "--------> NormalizeData completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####################################################################\n",
    "\n",
    "from transformers import NormalizeData\n",
    "\n",
    "NormalizeData = NormalizeData()\n",
    "\n",
    "NormalizeData.fit(window_size=25, shuffle=False, debug=False,export_excel = True, excel_path = excel_reports)\n",
    "\n",
    "data_normalized, Dates = NormalizeData.transform(data_prep)\n",
    "\n",
    "#Get only forecasts for model testing\n",
    "# x_valid_x = data_normalized[['maxv','minv']].copy() #extreme values for reverting normalization\n",
    "# x_valid = data_normalized.iloc[:,:-2].copy() #dataset for forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split ratio: 80 %\n",
      "train period: 2019-12-30 - 2022-02-21\n",
      "valid period: 2021-09-20 - 2022-08-01\n",
      "x_train window:  90.0\n",
      "x_valid window:  23.0\n",
      "--------> SplitData completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####################################################################\n",
    "\n",
    "from training import SplitData\n",
    "\n",
    "split_ratio = 0.80\n",
    "\n",
    "SplitData = SplitData()\n",
    "\n",
    "SplitData.fit(split_ratio=split_ratio, window_size=25,\n",
    "              dates=Dates, debug=False, export_excel=True)\n",
    "\n",
    "x_train, x_valid, x_train_x, x_valid_x = SplitData.transform(data_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------> GetTensoredDataset completed\n",
      "\n",
      "(<tf.Tensor: shape=(8, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.25195665, 0.29716039, 0.24809357, ..., 0.32116699,\n",
      "         0.3623027 , 0.50274895],\n",
      "        [0.25908094, 0.34050777, 0.25381301, ..., 0.32116699,\n",
      "         0.3623027 , 0.50274895],\n",
      "        [0.33604259, 0.36278351, 0.32033917, ..., 0.32116699,\n",
      "         0.3623027 , 0.50274895],\n",
      "        ...,\n",
      "        [0.93111574, 0.9361328 , 0.69832429, ..., 0.74148883,\n",
      "         0.62341116, 0.50274895],\n",
      "        [0.866195  , 0.89905682, 0.83102553, ..., 0.78209322,\n",
      "         0.66344086, 0.50274895],\n",
      "        [0.87502507, 1.        , 0.83659439, ..., 0.81181308,\n",
      "         0.69769807, 0.50274895]],\n",
      "\n",
      "       [[0.25187791, 0.33104089, 0.24675645, ..., 0.31223783,\n",
      "         0.35222988, 0.48877141],\n",
      "        [0.32669986, 0.35269733, 0.31143303, ..., 0.31223783,\n",
      "         0.35222988, 0.48877141],\n",
      "        [0.32743146, 0.3844503 , 0.32138332, ..., 0.31223783,\n",
      "         0.35222988, 0.48877141],\n",
      "        ...,\n",
      "        [0.84211285, 0.87406104, 0.80792117, ..., 0.76034929,\n",
      "         0.64499573, 0.48877141],\n",
      "        [0.85069743, 0.97219778, 0.81333521, ..., 0.78924287,\n",
      "         0.67830051, 0.48877141],\n",
      "        [0.838601  , 0.96146706, 0.82465121, ..., 0.82820691,\n",
      "         0.71634921, 0.52371906]],\n",
      "\n",
      "       [[0.32075476, 0.34627914, 0.30576575, ..., 0.30655591,\n",
      "         0.3458202 , 0.47987703],\n",
      "        [0.32147305, 0.37745429, 0.31553497, ..., 0.30655591,\n",
      "         0.3458202 , 0.47987703],\n",
      "        [0.27564415, 0.32846475, 0.24686333, ..., 0.30655591,\n",
      "         0.3458202 , 0.47987703],\n",
      "        ...,\n",
      "        [0.83521692, 0.95450627, 0.7985346 , ..., 0.77488068,\n",
      "         0.66595719, 0.47987703],\n",
      "        [0.82334061, 0.94397083, 0.80964468, ..., 0.81313568,\n",
      "         0.70331349, 0.51418872],\n",
      "        [0.91619577, 0.98180257, 0.88267408, ..., 0.83464569,\n",
      "         0.73179152, 0.54412728]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.21919446, 0.29309591, 0.21819849, ..., 0.2550297 ,\n",
      "         0.28769441, 0.39921884],\n",
      "        [0.28612408, 0.45532052, 0.28532731, ..., 0.30635948,\n",
      "         0.28769441, 0.39921884],\n",
      "        [0.41815074, 0.54037691, 0.41504318, ..., 0.34789535,\n",
      "         0.28769441, 0.39921884],\n",
      "        ...,\n",
      "        [0.7414446 , 0.83191904, 0.69845823, ..., 0.72795821,\n",
      "         0.64004797, 0.48141303],\n",
      "        [0.83259636, 0.97816816, 0.82793514, ..., 0.79143345,\n",
      "         0.6877516 , 0.51890971],\n",
      "        [0.96856693, 1.        , 0.83956819, ..., 0.82412684,\n",
      "         0.72130679, 0.54986576]],\n",
      "\n",
      "       [[0.28612408, 0.45532052, 0.28532731, ..., 0.30635948,\n",
      "         0.28769441, 0.39921884],\n",
      "        [0.41815074, 0.54037691, 0.41504318, ..., 0.34789535,\n",
      "         0.28769441, 0.39921884],\n",
      "        [0.35659936, 0.40313134, 0.24345643, ..., 0.35024563,\n",
      "         0.28769441, 0.39921884],\n",
      "        ...,\n",
      "        [0.83259636, 0.97816816, 0.82793514, ..., 0.79143345,\n",
      "         0.6877516 , 0.51890971],\n",
      "        [0.96856693, 1.        , 0.83956819, ..., 0.82412684,\n",
      "         0.72130679, 0.54986576],\n",
      "        [0.91745349, 0.96976218, 0.83821356, ..., 0.84716055,\n",
      "         0.74952802, 0.57825608]],\n",
      "\n",
      "       [[0.37551436, 0.48527784, 0.37272366, ..., 0.3124225 ,\n",
      "         0.25835989, 0.35851283],\n",
      "        [0.32023901, 0.3620264 , 0.2186326 , ..., 0.31453313,\n",
      "         0.25835989, 0.35851283],\n",
      "        [0.34424528, 0.37283109, 0.27662697, ..., 0.31192127,\n",
      "         0.25835989, 0.35851283],\n",
      "        ...,\n",
      "        [0.86980783, 0.89803585, 0.75396234, ..., 0.74009545,\n",
      "         0.64775935, 0.49379916],\n",
      "        [0.82390612, 0.8708812 , 0.75274583, ..., 0.76078055,\n",
      "         0.67310304, 0.51929469],\n",
      "        [0.81789563, 0.89435084, 0.81460411, ..., 0.79273874,\n",
      "         0.70380014, 0.54756186]]])>, <tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
      "array([[0.98896241],\n",
      "       [1.        ],\n",
      "       [1.        ],\n",
      "       [1.        ],\n",
      "       [1.        ],\n",
      "       [0.96976218],\n",
      "       [0.99589658],\n",
      "       [1.        ]])>)\n",
      "(<tf.Tensor: shape=(8, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.31059374, 0.35112253, 0.21204761, ..., 0.30505971,\n",
      "         0.25057835, 0.34771478],\n",
      "        [0.33387696, 0.36160179, 0.26829525, ..., 0.30252652,\n",
      "         0.25057835, 0.34771478],\n",
      "        [0.20548944, 0.27898259, 0.12186408, ..., 0.27573356,\n",
      "         0.25057835, 0.34771478],\n",
      "        ...,\n",
      "        [0.7990909 , 0.84465114, 0.73007388, ..., 0.7378666 ,\n",
      "         0.65282985, 0.50365406],\n",
      "        [0.79326144, 0.86741389, 0.79006905, ..., 0.76886225,\n",
      "         0.68260239, 0.53106985],\n",
      "        [0.86269473, 0.96988101, 0.86030046, ..., 0.81419108,\n",
      "         0.72028097, 0.56278531]],\n",
      "\n",
      "       [[0.29010766, 0.31419793, 0.23312332, ..., 0.26286707,\n",
      "         0.21772901, 0.30213142],\n",
      "        [0.17855098, 0.24240962, 0.10588842, ..., 0.23958652,\n",
      "         0.21772901, 0.30213142],\n",
      "        [0.09566742, 0.14966689, 0.        , ..., 0.19272957,\n",
      "         0.21772901, 0.30213142],\n",
      "        ...,\n",
      "        [0.68926954, 0.75370104, 0.68649566, ..., 0.66806894,\n",
      "         0.59311724, 0.46144972],\n",
      "        [0.74960053, 0.84273532, 0.74752014, ..., 0.70745543,\n",
      "         0.62585638, 0.48900747],\n",
      "        [0.82217269, 0.86890589, 0.75722859, ..., 0.74814844,\n",
      "         0.6603217 , 0.51787735]],\n",
      "\n",
      "       [[0.17153286, 0.23288146, 0.10172637, ..., 0.23016933,\n",
      "         0.20917095, 0.29025584],\n",
      "        [0.09190711, 0.14378408, 0.        , ..., 0.18515414,\n",
      "         0.20917095, 0.29025584],\n",
      "        [0.07250033, 0.23879042, 0.05167423, ..., 0.19188022,\n",
      "         0.20909779, 0.29025584],\n",
      "        ...,\n",
      "        [0.72013672, 0.80961076, 0.71813811, ..., 0.67964817,\n",
      "         0.60125646, 0.46978654],\n",
      "        [0.78985636, 0.83475267, 0.72746495, ..., 0.71874171,\n",
      "         0.63436709, 0.49752166],\n",
      "        [0.8497567 , 0.96069399, 0.84581745, ..., 0.78372476,\n",
      "         0.68233867, 0.53341452]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.0483917 , 0.10553779, 0.        , ..., 0.02188141,\n",
      "         0.03735776, 0.12118717],\n",
      "        [0.04804952, 0.11366488, 0.04146228, ..., 0.03563969,\n",
      "         0.04238509, 0.12118717],\n",
      "        [0.06319148, 0.17283563, 0.05306832, ..., 0.06939605,\n",
      "         0.05952383, 0.12118717],\n",
      "        ...,\n",
      "        [0.78977977, 0.81992123, 0.74686319, ..., 0.66782721,\n",
      "         0.55529617, 0.39731929],\n",
      "        [0.823885  , 1.        , 0.65529827, ..., 0.69415674,\n",
      "         0.58678608, 0.4263322 ],\n",
      "        [0.65869169, 0.81048243, 0.65524127, ..., 0.69803744,\n",
      "         0.60539425, 0.44884476]],\n",
      "\n",
      "       [[0.01286846, 0.08090875, 0.00603778, ..., 0.        ,\n",
      "         0.00699469, 0.08870904],\n",
      "        [0.02857002, 0.14226627, 0.01807274, ..., 0.03500389,\n",
      "         0.02476683, 0.08870904],\n",
      "        [0.10749205, 0.12600283, 0.04796786, ..., 0.0577002 ,\n",
      "         0.03856285, 0.08870904],\n",
      "        ...,\n",
      "        [0.81737635, 1.        , 0.64255919, ..., 0.68285375,\n",
      "         0.57151501, 0.40513126],\n",
      "        [0.64607802, 0.80347847, 0.64250009, ..., 0.68687787,\n",
      "         0.59081088, 0.42847581],\n",
      "        [0.80534138, 0.83301875, 0.67615054, ..., 0.69058866,\n",
      "         0.60758853, 0.450187  ]],\n",
      "\n",
      "       [[0.01069048, 0.12647935, 0.        , ..., 0.01724277,\n",
      "         0.00681729, 0.07193639],\n",
      "        [0.09106511, 0.10991658, 0.03044534, ..., 0.04035682,\n",
      "         0.02086724, 0.07193639],\n",
      "        [0.11440356, 0.14213874, 0.07179208, ..., 0.05100751,\n",
      "         0.02960062, 0.07193639],\n",
      "        ...,\n",
      "        [0.63956395, 0.79986141, 0.63592016, ..., 0.68111474,\n",
      "         0.5832796 , 0.41795669],\n",
      "        [0.80175862, 0.8299454 , 0.67018997, ..., 0.68489383,\n",
      "         0.60036605, 0.44006748],\n",
      "        [0.6625109 , 0.77796856, 0.66004152, ..., 0.71115104,\n",
      "         0.62750882, 0.46700561]]])>, <tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
      "array([[1.        ],\n",
      "       [1.        ],\n",
      "       [1.        ],\n",
      "       [1.        ],\n",
      "       [0.82982967],\n",
      "       [0.83896991],\n",
      "       [0.78198128],\n",
      "       [0.88387991]])>)\n",
      "(<tf.Tensor: shape=(8, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.07169392, 0.09094716, 0.00978223, ..., 0.01990494,\n",
      "         0.        , 0.05215753],\n",
      "        [0.09552976, 0.12385603, 0.05201015, ..., 0.03078262,\n",
      "         0.00891951, 0.05215753],\n",
      "        [0.0521332 , 0.15015231, 0.0521332 , ..., 0.06466856,\n",
      "         0.03052934, 0.05215753],\n",
      "        ...,\n",
      "        [0.79753371, 0.8263212 , 0.66316107, ..., 0.6781783 ,\n",
      "         0.59184907, 0.42813422],\n",
      "        [0.65531835, 0.77323664, 0.65279634, ..., 0.7049951 ,\n",
      "         0.61957031, 0.45564645],\n",
      "        [0.79252048, 0.88140517, 0.75681289, ..., 0.73077565,\n",
      "         0.64659442, 0.4828129 ]],\n",
      "\n",
      "       [[0.08738973, 0.11597093, 0.04347845, ..., 0.02205987,\n",
      "         0.        , 0.04362716],\n",
      "        [0.0436026 , 0.14250387, 0.0436026 , ..., 0.05625078,\n",
      "         0.02180432, 0.04362716],\n",
      "        [0.14110736, 0.22713006, 0.11469858, ..., 0.10472765,\n",
      "         0.0532067 , 0.04362716],\n",
      "        ...,\n",
      "        [0.65221629, 0.77119582, 0.64967158, ..., 0.70234012,\n",
      "         0.61614652, 0.45074739],\n",
      "        [0.7906532 , 0.88033784, 0.75462426, ..., 0.72835269,\n",
      "         0.64341384, 0.47815833],\n",
      "        [0.81495188, 0.92741442, 0.81330705, ..., 0.77177702,\n",
      "         0.67986369, 0.51033269]],\n",
      "\n",
      "       [[0.02228418, 0.12338999, 0.02228418, ..., 0.03521429,\n",
      "         0.        , 0.02230928],\n",
      "        [0.12196235, 0.20990253, 0.09496491, ..., 0.08477173,\n",
      "         0.03210236, 0.02230928],\n",
      "        [0.24289603, 0.28508951, 0.23274417, ..., 0.13958524,\n",
      "         0.06972031, 0.02230928],\n",
      "        ...,\n",
      "        [0.78598679, 0.87767053, 0.74915475, ..., 0.72229758,\n",
      "         0.63546542, 0.4665263 ],\n",
      "        [0.81082709, 0.92579647, 0.8091456 , ..., 0.76668985,\n",
      "         0.67272774, 0.49941784],\n",
      "        [0.90644463, 0.95200089, 0.87830506, ..., 0.80016613,\n",
      "         0.70520914, 0.53017296]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.27705073, 0.28029556, 0.12649041, ..., 0.15440753,\n",
      "         0.07803956, 0.        ],\n",
      "        [0.23506256, 0.25631628, 0.21231633, ..., 0.18066885,\n",
      "         0.10392918, 0.        ],\n",
      "        [0.2407735 , 0.32160235, 0.21591805, ..., 0.19989048,\n",
      "         0.12608539, 0.        ],\n",
      "        ...,\n",
      "        [0.89308265, 0.90353112, 0.80595891, ..., 0.81185868,\n",
      "         0.72217592, 0.54609528],\n",
      "        [0.83130107, 0.86579368, 0.68502385, ..., 0.78429776,\n",
      "         0.72113277, 0.55963929],\n",
      "        [0.73145743, 0.98208845, 0.69735419, ..., 0.83981738,\n",
      "         0.76074565, 0.59315747]],\n",
      "\n",
      "       [[0.23506256, 0.25631628, 0.21231633, ..., 0.18066885,\n",
      "         0.10392918, 0.        ],\n",
      "        [0.2407735 , 0.32160235, 0.21591805, ..., 0.19989048,\n",
      "         0.12608539, 0.        ],\n",
      "        [0.23272627, 0.31446367, 0.22344609, ..., 0.22581154,\n",
      "         0.15139752, 0.02324914],\n",
      "        ...,\n",
      "        [0.83130107, 0.86579368, 0.68502385, ..., 0.78429776,\n",
      "         0.72113277, 0.55963929],\n",
      "        [0.73145743, 0.98208845, 0.69735419, ..., 0.83981738,\n",
      "         0.76074565, 0.59315747],\n",
      "        [0.9773834 , 0.99542467, 0.72762855, ..., 0.83256324,\n",
      "         0.76900445, 0.6108591 ]],\n",
      "\n",
      "       [[0.2407735 , 0.32160235, 0.21591805, ..., 0.19989048,\n",
      "         0.12608539, 0.        ],\n",
      "        [0.23272627, 0.31446367, 0.22344609, ..., 0.22581154,\n",
      "         0.15139752, 0.02324914],\n",
      "        [0.29564367, 0.34009794, 0.27292982, ..., 0.24038644,\n",
      "         0.17069385, 0.0435351 ],\n",
      "        ...,\n",
      "        [0.73145743, 0.98208845, 0.69735419, ..., 0.83981738,\n",
      "         0.76074565, 0.59315747],\n",
      "        [0.9773834 , 0.99542467, 0.72762855, ..., 0.83256324,\n",
      "         0.76900445, 0.6108591 ],\n",
      "        [0.79826867, 0.86017998, 0.78431572, ..., 0.81962192,\n",
      "         0.77181433, 0.62497186]]])>, <tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
      "array([[0.92806185],\n",
      "       [0.95304748],\n",
      "       [0.90568327],\n",
      "       [0.86579368],\n",
      "       [0.98208845],\n",
      "       [0.99542467],\n",
      "       [0.86017998],\n",
      "       [0.82877   ]])>)\n",
      "(<tf.Tensor: shape=(8, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.21446322, 0.29814617, 0.20496215, ..., 0.2073839 ,\n",
      "         0.13119864, 0.        ],\n",
      "        [0.27887821, 0.32439061, 0.25562372, ..., 0.22230571,\n",
      "         0.15095427, 0.02076882],\n",
      "        [0.26157025, 0.33701444, 0.22572509, ..., 0.25032432,\n",
      "         0.17701836, 0.04473698],\n",
      "        ...,\n",
      "        [0.97684507, 0.99531577, 0.72114542, ..., 0.82857782,\n",
      "         0.76350618, 0.60159656],\n",
      "        [0.79346696, 0.85685191, 0.77918189, ..., 0.81532847,\n",
      "         0.76638293, 0.61604525],\n",
      "        [0.79330069, 0.82469429, 0.75526315, ..., 0.81245184,\n",
      "         0.77236406, 0.63118245]],\n",
      "\n",
      "       [[0.26358371, 0.3100614 , 0.23983601, ..., 0.20581135,\n",
      "         0.13294659, 0.        ],\n",
      "        [0.24590866, 0.32295297, 0.20930325, ..., 0.23442421,\n",
      "         0.15956348, 0.02447651],\n",
      "        [0.32352974, 0.44749268, 0.31956044, ..., 0.28847711,\n",
      "         0.20018592, 0.05640714],\n",
      "        ...,\n",
      "        [0.78908653, 0.85381584, 0.77449849, ..., 0.81141172,\n",
      "         0.76142807, 0.60790183],\n",
      "        [0.78891674, 0.82097618, 0.75007245, ..., 0.80847407,\n",
      "         0.76753606, 0.62336008],\n",
      "        [0.80163872, 0.85707274, 0.76191234, ..., 0.81789097,\n",
      "         0.77890485, 0.64080593]],\n",
      "\n",
      "       [[0.22698803, 0.30596542, 0.18946416, ..., 0.21521543,\n",
      "         0.13847639, 0.        ],\n",
      "        [0.30655667, 0.43362991, 0.30248777, ..., 0.27062454,\n",
      "         0.18011808, 0.03273179],\n",
      "        [0.42524877, 0.45268747, 0.31264256, ..., 0.29916342,\n",
      "         0.20940923, 0.05975409],\n",
      "        ...,\n",
      "        [0.78362053, 0.81648436, 0.74380161, ..., 0.80366857,\n",
      "         0.76170339, 0.61390994],\n",
      "        [0.79666171, 0.8534866 , 0.75593857, ..., 0.81332174,\n",
      "         0.77335743, 0.63179352],\n",
      "        [0.84343614, 0.86151996, 0.73517689, ..., 0.79855609,\n",
      "         0.77155506, 0.6421814 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.32343608, 0.37331416, 0.24787816, ..., 0.25640941,\n",
      "         0.16331353, 0.        ],\n",
      "        [0.31705414, 0.3982342 , 0.31355921, ..., 0.29034259,\n",
      "         0.19590768, 0.03001404],\n",
      "        [0.39306779, 0.51041236, 0.39044661, ..., 0.33996734,\n",
      "         0.23715715, 0.06473526],\n",
      "        ...,\n",
      "        [0.75098958, 0.80812325, 0.74833039, ..., 0.77966684,\n",
      "         0.75484264, 0.62272406],\n",
      "        [0.74897611, 0.79383978, 0.72831071, ..., 0.76738148,\n",
      "         0.75204656, 0.63183959],\n",
      "        [0.74711475, 0.77009736, 0.70164327, ..., 0.76126539,\n",
      "         0.75111249, 0.64097043]],\n",
      "\n",
      "       [[0.29592191, 0.3796139 , 0.29231884, ..., 0.26838383,\n",
      "         0.17102685, 0.        ],\n",
      "        [0.37428763, 0.49526317, 0.37158535, ..., 0.3195441 ,\n",
      "         0.21355268, 0.03579558],\n",
      "        [0.46855376, 0.5292569 , 0.38419594, ..., 0.37240146,\n",
      "         0.25832071, 0.07329553],\n",
      "        ...,\n",
      "        [0.74120874, 0.78746061, 0.71990389, ..., 0.76018362,\n",
      "         0.74437419, 0.62044769],\n",
      "        [0.73928978, 0.76298354, 0.69241128, ..., 0.75387828,\n",
      "         0.74341123, 0.62986107],\n",
      "        [0.74582999, 0.83171511, 0.66464451, ..., 0.75929956,\n",
      "         0.7479407 , 0.6413004 ]],\n",
      "\n",
      "       [[0.35105838, 0.47652508, 0.34825578, ..., 0.29428254,\n",
      "         0.18435624, 0.        ],\n",
      "        [0.44882409, 0.51178081, 0.36133454, ..., 0.34910219,\n",
      "         0.23078626, 0.03889211],\n",
      "        [0.5328205 , 0.68838446, 0.52729662, ..., 0.44022594,\n",
      "         0.29805534, 0.08922356],\n",
      "        ...,\n",
      "        [0.72961105, 0.75418442, 0.68099221, ..., 0.74474114,\n",
      "         0.73388551, 0.61611985],\n",
      "        [0.73639406, 0.82546762, 0.65219461, ..., 0.75036369,\n",
      "         0.73858313, 0.62798387],\n",
      "        [0.78749061, 0.87558939, 0.69484271, ..., 0.73501117,\n",
      "         0.73212878, 0.63347555]]])>, <tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
      "array([[0.86004117],\n",
      "       [0.86490947],\n",
      "       [0.82434388],\n",
      "       [0.8048813 ],\n",
      "       [0.77615704],\n",
      "       [0.83676602],\n",
      "       [0.88004274],\n",
      "       [0.8818037 ]])>)\n",
      "(<tf.Tensor: shape=(8, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.42652026, 0.49202457, 0.33549036, ..., 0.32276302,\n",
      "         0.19965932, 0.        ],\n",
      "        [0.51391566, 0.67577465, 0.50816825, ..., 0.41757417,\n",
      "         0.26965051, 0.05236816],\n",
      "        [0.6884529 , 0.73312265, 0.62485038, ..., 0.50771879,\n",
      "         0.34094741, 0.10682514],\n",
      "        ...,\n",
      "        [0.725727  , 0.81840501, 0.63812035, ..., 0.74026192,\n",
      "         0.72800465, 0.61292989],\n",
      "        [0.77889122, 0.87055501, 0.68249424, ..., 0.72428815,\n",
      "         0.72128913, 0.6186438 ],\n",
      "        [0.71085136, 0.87702078, 0.68173357, ..., 0.7540766 ,\n",
      "         0.73779045, 0.63543611]],\n",
      "\n",
      "       [[0.4870536 , 0.65785726, 0.48098857, ..., 0.38538807,\n",
      "         0.22928983, 0.        ],\n",
      "        [0.67123614, 0.71837444, 0.60411881, ..., 0.48051428,\n",
      "         0.30452675, 0.05746638],\n",
      "        [0.72457338, 1.        , 0.46092025, ..., 0.52169109,\n",
      "         0.35377388, 0.10283972],\n",
      "        ...,\n",
      "        [0.76667227, 0.8634016 , 0.66494819, ..., 0.70905173,\n",
      "         0.70588697, 0.59756924],\n",
      "        [0.69487239, 0.87022469, 0.66414549, ..., 0.74048635,\n",
      "         0.72330019, 0.61528953],\n",
      "        [0.83142615, 0.83976563, 0.6551816 , ..., 0.72610316,\n",
      "         0.71819942, 0.62127799]],\n",
      "\n",
      "       [[0.58833483, 0.63351968, 0.52399887, ..., 0.40551657,\n",
      "         0.23682206, 0.        ],\n",
      "        [0.63946174, 0.90347454, 0.38673453, ..., 0.444987  ,\n",
      "         0.28402837, 0.04349304],\n",
      "        [0.39182157, 0.61937005, 0.38664908, ..., 0.45080453,\n",
      "         0.31192375, 0.07724147],\n",
      "        ...,\n",
      "        [0.61099157, 0.77907718, 0.58153801, ..., 0.65471526,\n",
      "         0.63824131, 0.53470667],\n",
      "        [0.74188647, 0.74988036, 0.57294559, ..., 0.64092812,\n",
      "         0.63335192, 0.54044696],\n",
      "        [0.61731822, 0.76753538, 0.59158373, ..., 0.66045426,\n",
      "         0.64503156, 0.55395277]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.56716526, 0.61136475, 0.36085341, ..., 0.38391043,\n",
      "         0.25136303, 0.        ],\n",
      "        [0.34881192, 0.52986024, 0.3449397 , ..., 0.42508417,\n",
      "         0.29392541, 0.04224148],\n",
      "        [0.55946809, 0.69593907, 0.5046437 , ..., 0.46466687,\n",
      "         0.33541745, 0.08395205],\n",
      "        ...,\n",
      "        [0.68923366, 0.98465293, 0.68923366, ..., 0.69892998,\n",
      "         0.64327307, 0.52635855],\n",
      "        [0.9417755 , 1.        , 0.84936249, ..., 0.76088035,\n",
      "         0.68519356, 0.55751037],\n",
      "        [0.88709263, 0.95480875, 0.59271239, ..., 0.73972177,\n",
      "         0.6854446 , 0.56785556]],\n",
      "\n",
      "       [[0.32009158, 0.50912495, 0.31604858, ..., 0.39972778,\n",
      "         0.26278433, 0.        ],\n",
      "        [0.54003864, 0.6825286 , 0.48279625, ..., 0.44105626,\n",
      "         0.30610635, 0.0435502 ],\n",
      "        [0.5786441 , 0.75732336, 0.57603082, ..., 0.51004836,\n",
      "         0.36401747, 0.09466847],\n",
      "        ...,\n",
      "        [0.93920753, 1.        , 0.8427187 , ..., 0.75033409,\n",
      "         0.67130918, 0.53799457],\n",
      "        [0.8821129 , 0.95281561, 0.57474916, ..., 0.72824232,\n",
      "         0.67157129, 0.54879604],\n",
      "        [0.70466625, 0.71452714, 0.27162538, ..., 0.64185848,\n",
      "         0.63377553, 0.53896426]],\n",
      "\n",
      "       [[0.51909514, 0.66807312, 0.45924632, ..., 0.41560578,\n",
      "         0.27451117, 0.        ],\n",
      "        [0.55945843, 0.74627353, 0.55672616, ..., 0.4877393 ,\n",
      "         0.33505916, 0.05344585],\n",
      "        [0.7148285 , 0.78885341, 0.66910418, ..., 0.54213531,\n",
      "         0.38783857, 0.10342021],\n",
      "        ...,\n",
      "        [0.87674513, 0.95066715, 0.55538614, ..., 0.71586833,\n",
      "         0.65661688, 0.52825129],\n",
      "        [0.69121877, 0.70152866, 0.23846017, ..., 0.62555116,\n",
      "         0.61710017, 0.51797184],\n",
      "        [0.397387  , 0.52033227, 0.21521132, ..., 0.58428032,\n",
      "         0.59617756, 0.51502235]]])>, <tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
      "array([[0.84815681],\n",
      "       [0.85818391],\n",
      "       [1.        ],\n",
      "       [1.        ],\n",
      "       [0.9563231 ],\n",
      "       [0.72658594],\n",
      "       [0.5412219 ],\n",
      "       [0.61647209]])>)\n",
      "(<tf.Tensor: shape=(8, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.53458387, 0.73194722, 0.53169733, ..., 0.45881522,\n",
      "         0.29751421, 0.        ],\n",
      "        [0.69872669, 0.77693131, 0.65042061, ..., 0.51628262,\n",
      "         0.35327374, 0.05279609],\n",
      "        [0.67988338, 0.6974197 , 0.53365814, ..., 0.54356009,\n",
      "         0.39303989, 0.0975127 ],\n",
      "        ...,\n",
      "        [0.67378387, 0.68467589, 0.19546089, ..., 0.60440843,\n",
      "         0.59548027, 0.4907548 ],\n",
      "        [0.3633613 , 0.49324851, 0.17089933, ..., 0.56080729,\n",
      "         0.5733763 , 0.48763877],\n",
      "        [0.45371075, 0.59481673, 0.39876037, ..., 0.5290257 ,\n",
      "         0.55432944, 0.48459341]],\n",
      "\n",
      "       [[0.68193405, 0.76449771, 0.63093544, ..., 0.48932076,\n",
      "         0.31722594, 0.        ],\n",
      "        [0.66204043, 0.68055421, 0.50766477, ..., 0.51811863,\n",
      "         0.35920861, 0.04720907],\n",
      "        [0.55256885, 0.61368675, 0.29337839, ..., 0.46928309,\n",
      "         0.35736024, 0.07120788],\n",
      "        ...,\n",
      "        [0.32787577, 0.46500274, 0.12468618, ..., 0.53632718,\n",
      "         0.54959677, 0.45908033],\n",
      "        [0.4232612 , 0.57223226, 0.36524794, ..., 0.50277412,\n",
      "         0.52948826, 0.45586522],\n",
      "        [0.43430027, 0.54509449, 0.2869389 , ..., 0.47838053,\n",
      "         0.51224338, 0.45278773]],\n",
      "\n",
      "       [[0.64529515, 0.66472625, 0.48327045, ..., 0.49424228,\n",
      "         0.32745856, 0.        ],\n",
      "        [0.53039945, 0.59454563, 0.25836656, ..., 0.44298703,\n",
      "         0.32551861, 0.02518791],\n",
      "        [0.34471946, 0.81081993, 0.28129738, ..., 0.54623731,\n",
      "         0.39918698, 0.08752192],\n",
      "        ...,\n",
      "        [0.39468484, 0.55103715, 0.33379713, ..., 0.47813748,\n",
      "         0.50617525, 0.42890433],\n",
      "        [0.40627087, 0.52255474, 0.25160801, ..., 0.45253523,\n",
      "         0.48807592, 0.42567435],\n",
      "        [0.38394335, 0.63732964, 0.35491768, ..., 0.50131638,\n",
      "         0.5088749 , 0.44148194]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.64246124, 0.67236817, 0.22844387, ..., 0.40239359,\n",
      "         0.2970325 , 0.03487573],\n",
      "        [0.34554365, 0.44817371, 0.32241391, ..., 0.38094083,\n",
      "         0.30169041, 0.05827039],\n",
      "        [0.34527444, 0.39610555, 0.28368576, ..., 0.37628312,\n",
      "         0.31137479, 0.08277986],\n",
      "        ...,\n",
      "        [0.4950783 , 0.62503351, 0.46151373, ..., 0.44159809,\n",
      "         0.41627409, 0.34413112],\n",
      "        [0.58592879, 1.        , 0.55354742, ..., 0.5825918 ,\n",
      "         0.49608978, 0.39140672],\n",
      "        [0.85417683, 0.91248443, 0.69216299, ..., 0.64351273,\n",
      "         0.54220136, 0.42375939]],\n",
      "\n",
      "       [[0.34554365, 0.44817371, 0.32241391, ..., 0.38094083,\n",
      "         0.30169041, 0.05827039],\n",
      "        [0.34527444, 0.39610555, 0.28368576, ..., 0.37628312,\n",
      "         0.31137479, 0.08277986],\n",
      "        [0.36544549, 0.45333762, 0.30245823, ..., 0.39121387,\n",
      "         0.32940032, 0.11044074],\n",
      "        ...,\n",
      "        [0.58592879, 1.        , 0.55354742, ..., 0.5825918 ,\n",
      "         0.49608978, 0.39140672],\n",
      "        [0.85417683, 0.91248443, 0.69216299, ..., 0.64351273,\n",
      "         0.54220136, 0.42375939],\n",
      "        [0.77854886, 0.88300791, 0.73809907, ..., 0.67130582,\n",
      "         0.57275323, 0.44912172]],\n",
      "\n",
      "       [[0.34527444, 0.39610555, 0.28368576, ..., 0.37628312,\n",
      "         0.31137479, 0.08277986],\n",
      "        [0.36544549, 0.45333762, 0.30245823, ..., 0.39121387,\n",
      "         0.32940032, 0.11044074],\n",
      "        [0.43779239, 0.46576297, 0.27034575, ..., 0.36837556,\n",
      "         0.32661255, 0.12650786],\n",
      "        ...,\n",
      "        [0.85417683, 0.91248443, 0.69216299, ..., 0.64351273,\n",
      "         0.54220136, 0.42375939],\n",
      "        [0.77854886, 0.88300791, 0.73809907, ..., 0.67130582,\n",
      "         0.57275323, 0.44912172],\n",
      "        [0.76563925, 0.7918347 , 0.52579188, ..., 0.67903232,\n",
      "         0.59207559, 0.46905987]]])>, <tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
      "array([[0.56911172],\n",
      "       [0.65445096],\n",
      "       [0.78251837],\n",
      "       [1.        ],\n",
      "       [0.91248443],\n",
      "       [0.88300791],\n",
      "       [0.7918347 ],\n",
      "       [0.69700384]])>)\n",
      "(<tf.Tensor: shape=(8, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.36544549, 0.45333762, 0.30245823, ..., 0.39121387,\n",
      "         0.32940032, 0.11044074],\n",
      "        [0.43779239, 0.46576297, 0.27034575, ..., 0.36837556,\n",
      "         0.32661255, 0.12650786],\n",
      "        [0.32736278, 0.40826202, 0.32359745, ..., 0.36796876,\n",
      "         0.33281858, 0.14574337],\n",
      "        ...,\n",
      "        [0.77854886, 0.88300791, 0.73809907, ..., 0.67130582,\n",
      "         0.57275323, 0.44912172],\n",
      "        [0.76563925, 0.7918347 , 0.52579188, ..., 0.67903232,\n",
      "         0.59207559, 0.46905987],\n",
      "        [0.69296963, 0.69700384, 0.40718617, ..., 0.64955733,\n",
      "         0.5895824 , 0.47760467]],\n",
      "\n",
      "       [[0.43191465, 0.45950971, 0.26671613, ..., 0.3634298 ,\n",
      "         0.32222749, 0.12480938],\n",
      "        [0.32296766, 0.40278075, 0.31925288, ..., 0.36302846,\n",
      "         0.3283502 , 0.14378664],\n",
      "        [0.32015493, 0.3828274 , 0.29128631, ..., 0.34586638,\n",
      "         0.3244442 , 0.1565206 ],\n",
      "        ...,\n",
      "        [0.75535989, 0.78120364, 0.51873267, ..., 0.66991573,\n",
      "         0.58412647, 0.46276234],\n",
      "        [0.68366592, 0.68764597, 0.40171934, ..., 0.64083647,\n",
      "         0.58166675, 0.47119242],\n",
      "        [0.54988347, 0.77595003, 0.419603  , ..., 0.66547603,\n",
      "         0.60403724, 0.49166302]],\n",
      "\n",
      "       [[0.24965139, 0.31134627, 0.2467799 , ..., 0.28061807,\n",
      "         0.25381206, 0.11114591],\n",
      "        [0.24747718, 0.29592249, 0.22516197, ..., 0.26735192,\n",
      "         0.25079275, 0.12098916],\n",
      "        [0.24546721, 0.27028462, 0.19636556, ..., 0.26074756,\n",
      "         0.24978412, 0.13084896],\n",
      "        ...,\n",
      "        [0.52846824, 0.53154478, 0.31052581, ..., 0.49536142,\n",
      "         0.44962371, 0.36422794],\n",
      "        [0.42505549, 0.59980311, 0.32434973, ..., 0.5144076 ,\n",
      "         0.46691591, 0.38005154],\n",
      "        [0.59824431, 0.77299193, 0.59225521, ..., 0.58674184,\n",
      "         0.51317153, 0.41105362]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.19580436, 0.26561398, 0.12981473, ..., 0.20675278,\n",
      "         0.19752   , 0.11084005],\n",
      "        [0.23585026, 0.30489592, 0.16323932, ..., 0.19472055,\n",
      "         0.19246154, 0.11514404],\n",
      "        [0.1845993 , 0.30976626, 0.16266635, ..., 0.21715867,\n",
      "         0.20489114, 0.12779284],\n",
      "        ...,\n",
      "        [0.59890493, 0.77602346, 0.55220605, ..., 0.54394296,\n",
      "         0.45473371, 0.35470721],\n",
      "        [0.76459535, 0.81145354, 0.71426748, ..., 0.61622718,\n",
      "         0.50738048, 0.39008565],\n",
      "        [0.80721971, 0.99427011, 0.76628259, ..., 0.69745429,\n",
      "         0.5678638 , 0.43092057]],\n",
      "\n",
      "       [[0.20721575, 0.2678786 , 0.14342049, ..., 0.17107959,\n",
      "         0.16909484, 0.10116444],\n",
      "        [0.16218716, 0.27215763, 0.14291708, ..., 0.1907935 ,\n",
      "         0.18001537, 0.11227755],\n",
      "        [0.24782551, 0.25305553, 0.13729547, ..., 0.18177323,\n",
      "         0.17681648, 0.11603315],\n",
      "        ...,\n",
      "        [0.67176607, 0.71293523, 0.62754849, ..., 0.54141123,\n",
      "         0.44577958, 0.34272548],\n",
      "        [0.70921542, 0.87355609, 0.67324847, ..., 0.61277658,\n",
      "         0.49891964, 0.37860264],\n",
      "        [0.76842387, 0.87859032, 0.69990199, ..., 0.67628954,\n",
      "         0.55063537, 0.41512018]],\n",
      "\n",
      "       [[0.15574481, 0.26134707, 0.13724017, ..., 0.18321486,\n",
      "         0.17286486, 0.10781769],\n",
      "        [0.23798146, 0.24300374, 0.13184186, ..., 0.17455289,\n",
      "         0.16979303, 0.11142412],\n",
      "        [0.15971962, 0.25409576, 0.14355154, ..., 0.18682047,\n",
      "         0.17713093, 0.11990934],\n",
      "        ...,\n",
      "        [0.68104418, 0.83885696, 0.64650589, ..., 0.58843605,\n",
      "         0.4791017 , 0.3635639 ],\n",
      "        [0.73790077, 0.84369122, 0.67210069, ..., 0.64942617,\n",
      "         0.5287632 , 0.3986309 ],\n",
      "        [0.83737985, 0.9602783 , 0.83284098, ..., 0.73768087,\n",
      "         0.5948485 , 0.44340584]]])>, <tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
      "array([[0.78650959],\n",
      "       [1.        ],\n",
      "       [1.        ],\n",
      "       [1.        ],\n",
      "       [1.        ],\n",
      "       [1.        ],\n",
      "       [1.        ],\n",
      "       [1.        ]])>)\n",
      "(<tf.Tensor: shape=(8, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.23798146, 0.24300374, 0.13184186, ..., 0.17455289,\n",
      "         0.16979303, 0.11142412],\n",
      "        [0.15971962, 0.25409576, 0.14355154, ..., 0.18682047,\n",
      "         0.17713093, 0.11990934],\n",
      "        [0.23212662, 0.40014509, 0.23212662, ..., 0.23764136,\n",
      "         0.20598673, 0.13949208],\n",
      "        ...,\n",
      "        [0.73790077, 0.84369122, 0.67210069, ..., 0.64942617,\n",
      "         0.5287632 , 0.3986309 ],\n",
      "        [0.83737985, 0.9602783 , 0.83284098, ..., 0.73768087,\n",
      "         0.5948485 , 0.44340584],\n",
      "        [0.98522855, 1.        , 0.87385193, ..., 0.78731439,\n",
      "         0.64354845, 0.48084523]],\n",
      "\n",
      "       [[0.15971962, 0.25409576, 0.14355154, ..., 0.18682047,\n",
      "         0.17713093, 0.11990934],\n",
      "        [0.23212662, 0.40014509, 0.23212662, ..., 0.23764136,\n",
      "         0.20598673, 0.13949208],\n",
      "        [0.37575873, 0.40887367, 0.32319922, ..., 0.27287537,\n",
      "         0.22982883, 0.15720955],\n",
      "        ...,\n",
      "        [0.83737985, 0.9602783 , 0.83284098, ..., 0.73768087,\n",
      "         0.5948485 , 0.44340584],\n",
      "        [0.98522855, 1.        , 0.87385193, ..., 0.78731439,\n",
      "         0.64354845, 0.48084523],\n",
      "        [0.93178271, 0.96323252, 0.69898475, ..., 0.76477843,\n",
      "         0.65353154, 0.49905269]],\n",
      "\n",
      "       [[0.23212662, 0.40014509, 0.23212662, ..., 0.23764136,\n",
      "         0.20598673, 0.13949208],\n",
      "        [0.37575873, 0.40887367, 0.32319922, ..., 0.27287537,\n",
      "         0.22982883, 0.15720955],\n",
      "        [0.34465808, 0.38317133, 0.17723055, ..., 0.26084151,\n",
      "         0.22997161, 0.16309333],\n",
      "        ...,\n",
      "        [0.98522855, 1.        , 0.87385193, ..., 0.78731439,\n",
      "         0.64354845, 0.48084523],\n",
      "        [0.93178271, 0.96323252, 0.69898475, ..., 0.76477843,\n",
      "         0.65353154, 0.49905269],\n",
      "        [0.68203788, 0.8938604 , 0.67669333, ..., 0.79156072,\n",
      "         0.68506768, 0.52780979]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.24799913, 0.25337057, 0.01211261, ..., 0.21378641,\n",
      "         0.20938346, 0.15773776],\n",
      "        [0.09491324, 0.15896754, 0.        , ..., 0.19228437,\n",
      "         0.19848281, 0.15620108],\n",
      "        [0.13946937, 0.20905623, 0.11237041, ..., 0.17661118,\n",
      "         0.18908979, 0.15469925],\n",
      "        ...,\n",
      "        [0.83198153, 0.89020786, 0.7707472 , ..., 0.80887999,\n",
      "         0.71077698, 0.55375926],\n",
      "        [0.87377128, 0.98474514, 0.82220548, ..., 0.84786252,\n",
      "         0.74686035, 0.58508403],\n",
      "        [0.95391316, 0.96057369, 0.84476557, ..., 0.87024367,\n",
      "         0.77445053, 0.61237303]],\n",
      "\n",
      "       [[0.07919859, 0.13264751, 0.        , ..., 0.16044812,\n",
      "         0.1656203 , 0.13033909],\n",
      "        [0.11637762, 0.17444309, 0.0937654 , ..., 0.14736992,\n",
      "         0.15778247, 0.12908591],\n",
      "        [0.1206804 , 0.16386541, 0.06324235, ..., 0.13786186,\n",
      "         0.15106081, 0.12788638],\n",
      "        ...,\n",
      "        [0.72910223, 0.82170231, 0.6860741 , ..., 0.70748314,\n",
      "         0.62320375, 0.4882125 ],\n",
      "        [0.79597513, 0.80153288, 0.70489895, ..., 0.72615867,\n",
      "         0.64622587, 0.51098331],\n",
      "        [0.7680965 , 0.83353496, 0.6450181 , ..., 0.75558265,\n",
      "         0.6743669 , 0.53643605]],\n",
      "\n",
      "       [[0.05503209, 0.11517036, 0.03161265, ..., 0.08713074,\n",
      "         0.09791499, 0.06819404],\n",
      "        [0.05948846, 0.10421509, 0.        , ..., 0.07728326,\n",
      "         0.09095339, 0.06695168],\n",
      "        [0.05090055, 0.14836131, 0.03973632, ..., 0.09604611,\n",
      "         0.09895337, 0.0730318 ],\n",
      "        ...,\n",
      "        [0.75888972, 0.76464587, 0.66456232, ..., 0.68658097,\n",
      "         0.60379474, 0.46372432],\n",
      "        [0.73001589, 0.79779036, 0.60254386, ..., 0.71705532,\n",
      "         0.63294035, 0.49008567],\n",
      "        [0.80763172, 0.97019783, 0.80568198, ..., 0.78710049,\n",
      "         0.68359775, 0.52785589]]])>, <tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
      "array([[0.96323252],\n",
      "       [0.8938604 ],\n",
      "       [0.89020786],\n",
      "       [0.98474514],\n",
      "       [0.96057369],\n",
      "       [0.99892565],\n",
      "       [1.        ],\n",
      "       [1.        ]])>)\n",
      "(<tf.Tensor: shape=(8, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.05948846, 0.10421509, 0.        , ..., 0.07728326,\n",
      "         0.09095339, 0.06695168],\n",
      "        [0.05090055, 0.14836131, 0.03973632, ..., 0.09604611,\n",
      "         0.09895337, 0.0730318 ],\n",
      "        [0.14812926, 0.20420571, 0.13364595, ..., 0.12505223,\n",
      "         0.11412478, 0.08299466],\n",
      "        ...,\n",
      "        [0.73001589, 0.79779036, 0.60254386, ..., 0.71705532,\n",
      "         0.63294035, 0.49008567],\n",
      "        [0.80763172, 0.97019783, 0.80568198, ..., 0.78710049,\n",
      "         0.68359775, 0.52785589],\n",
      "        [0.98022477, 1.        , 0.91430697, ..., 0.84262367,\n",
      "         0.72941834, 0.56414195]],\n",
      "\n",
      "       [[0.01162622, 0.11311996, 0.        , ..., 0.05863992,\n",
      "         0.06166748, 0.03467327],\n",
      "        [0.11287831, 0.17127524, 0.09779567, ..., 0.08884634,\n",
      "         0.0774667 , 0.0450484 ],\n",
      "        [0.15370306, 0.3397709 , 0.13915211, ..., 0.15220355,\n",
      "         0.11333283, 0.06629225],\n",
      "        ...,\n",
      "        [0.7996714 , 0.9689646 , 0.79764097, ..., 0.77829058,\n",
      "         0.67050482, 0.50831827],\n",
      "        [0.97940646, 1.        , 0.91076094, ..., 0.83611134,\n",
      "         0.7182215 , 0.54610587],\n",
      "        [0.9796965 , 0.98607766, 0.89693523, ..., 0.86732917,\n",
      "         0.753168  , 0.57804729]],\n",
      "\n",
      "       [[0.07102969, 0.1321814 , 0.05523554, ..., 0.04586404,\n",
      "         0.03394758, 0.        ],\n",
      "        [0.11378028, 0.30862559, 0.09854291, ..., 0.11221003,\n",
      "         0.07150565, 0.022246  ],\n",
      "        [0.24000706, 0.26744427, 0.16376984, ..., 0.14087698,\n",
      "         0.09320391, 0.03746987],\n",
      "        ...,\n",
      "        [0.97843499, 1.        , 0.90655122, ..., 0.82838014,\n",
      "         0.70492903, 0.52469409],\n",
      "        [0.97873871, 0.9854209 , 0.89207331, ..., 0.86107062,\n",
      "         0.74152407, 0.55814231],\n",
      "        [0.9636532 , 0.99200174, 0.87719033, ..., 0.86770114,\n",
      "         0.76348613, 0.58423312]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.18009193, 0.23074923, 0.16047585, ..., 0.12808454,\n",
      "         0.08029157, 0.0203366 ],\n",
      "        [0.17383143, 0.18653488, 0.05751771, ..., 0.13183149,\n",
      "         0.08966192, 0.03000558],\n",
      "        [0.13859038, 0.14054676, 0.        , ..., 0.11753763,\n",
      "         0.08845285, 0.03414938],\n",
      "        ...,\n",
      "        [0.80206608, 0.94699513, 0.75156512, ..., 0.87393243,\n",
      "         0.7783109 , 0.59776799],\n",
      "        [0.86091416, 0.87020046, 0.70367284, ..., 0.84135528,\n",
      "         0.77548036, 0.61073954],\n",
      "        [0.7350793 , 0.82042997, 0.63606019, ..., 0.82073912,\n",
      "         0.77451395, 0.62341628]],\n",
      "\n",
      "       [[0.17226763, 0.1848568 , 0.05700027, ..., 0.13064553,\n",
      "         0.08885532, 0.02973565],\n",
      "        [0.13734361, 0.13928239, 0.        , ..., 0.11648025,\n",
      "         0.08765713, 0.03384216],\n",
      "        [0.07217465, 0.18229763, 0.0087116 , ..., 0.12848283,\n",
      "         0.09855438, 0.04381393],\n",
      "        ...,\n",
      "        [0.85316931, 0.86237207, 0.69734255, ..., 0.83378639,\n",
      "         0.76850408, 0.60524529],\n",
      "        [0.72846647, 0.81304932, 0.63033815, ..., 0.81335569,\n",
      "         0.76754637, 0.61780798],\n",
      "        [0.73580807, 0.87602105, 0.72919036, ..., 0.82922157,\n",
      "         0.78313712, 0.63789424]],\n",
      "\n",
      "       [[0.10842858, 0.10995919, 0.        , ..., 0.0919576 ,\n",
      "         0.06920262, 0.02671735],\n",
      "        [0.05697968, 0.14391841, 0.00687754, ..., 0.10143327,\n",
      "         0.07780567, 0.03458976],\n",
      "        [0.14314289, 0.23008161, 0.14016325, ..., 0.13742029,\n",
      "         0.10081831, 0.05001361],\n",
      "        ...,\n",
      "        [0.57510202, 0.64187759, 0.49763271, ..., 0.64211946,\n",
      "         0.6059544 , 0.48774052],\n",
      "        [0.58089799, 0.69159185, 0.5756735 , ..., 0.65464509,\n",
      "         0.61826283, 0.50359801],\n",
      "        [0.67673477, 0.78946944, 0.66816332, ..., 0.68374358,\n",
      "         0.63952851, 0.52382935]]])>, <tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
      "array([[0.98663089],\n",
      "       [0.99236205],\n",
      "       [0.94856785],\n",
      "       [0.87118618],\n",
      "       [0.82042997],\n",
      "       [0.88397334],\n",
      "       [1.        ],\n",
      "       [1.        ]])>)\n",
      "(<tf.Tensor: shape=(8, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.03437082, 0.0940121 , 0.        , ..., 0.06486666,\n",
      "         0.04865777, 0.019011  ],\n",
      "        [0.09348008, 0.15312136, 0.09143601, ..., 0.08955429,\n",
      "         0.06444479, 0.02959199],\n",
      "        [0.15270137, 0.2305991 , 0.13216294, ..., 0.12852878,\n",
      "         0.08929405, 0.04530183],\n",
      "        ...,\n",
      "        [0.39378668, 0.46972433, 0.3902026 , ..., 0.44437831,\n",
      "         0.41941952, 0.34075773],\n",
      "        [0.45953216, 0.53686984, 0.45365201, ..., 0.4643403 ,\n",
      "         0.4340081 , 0.35463674],\n",
      "        [0.52807761, 0.68129698, 0.51670936, ..., 0.52405588,\n",
      "         0.47082914, 0.38013338]],\n",
      "\n",
      "       [[0.06255142, 0.12094487, 0.06055011, ..., 0.05870777,\n",
      "         0.0341236 , 0.        ],\n",
      "        [0.12053367, 0.19680162, 0.10042495, ..., 0.09686683,\n",
      "         0.05845297, 0.01538116],\n",
      "        [0.19188063, 0.21205795, 0.17020925, ..., 0.1279927 ,\n",
      "         0.08112288, 0.03061526],\n",
      "        ...,\n",
      "        [0.42094492, 0.49666454, 0.4151878 , ..., 0.42565247,\n",
      "         0.39595489, 0.31824413],\n",
      "        [0.48805626, 0.63806996, 0.47692587, ..., 0.48411867,\n",
      "         0.43200555, 0.34320734],\n",
      "        [0.63478016, 0.95010506, 0.61164216, ..., 0.59144073,\n",
      "         0.49781175, 0.38453042]],\n",
      "\n",
      "       [[0.1023605 , 0.17660339, 0.08278571, ..., 0.07932206,\n",
      "         0.04192817, 0.        ],\n",
      "        [0.17181306, 0.19145463, 0.15071709, ..., 0.10962148,\n",
      "         0.06399614, 0.0148296 ],\n",
      "        [0.18967993, 0.26808596, 0.1725203 , ..., 0.1436695 ,\n",
      "         0.08934897, 0.0319464 ],\n",
      "        ...,\n",
      "        [0.46012463, 0.60615517, 0.44928977, ..., 0.4562916 ,\n",
      "         0.40556218, 0.31912174],\n",
      "        [0.60295272, 0.90990511, 0.58042907, ..., 0.56076404,\n",
      "         0.46962109, 0.35934761],\n",
      "        [0.84313452, 0.95847524, 0.76995917, ..., 0.64511649,\n",
      "         0.52906364, 0.39907961]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.16304176, 0.21388111, 0.13142044, ..., 0.12052383,\n",
      "         0.06253723, 0.        ],\n",
      "        [0.21084808, 0.26990897, 0.20866685, ..., 0.16293608,\n",
      "         0.09429562, 0.02151734],\n",
      "        [0.28189923, 0.28899789, 0.22837535, ..., 0.1867883 ,\n",
      "         0.11769919, 0.03950946],\n",
      "        ...,\n",
      "        [0.78858884, 0.91951409, 0.69813902, ..., 0.67280404,\n",
      "         0.53325506, 0.38433836],\n",
      "        [0.94166196, 1.        , 0.80552237, ..., 0.71991313,\n",
      "         0.58009056, 0.42060616],\n",
      "        [0.88223964, 0.93624107, 0.76680239, ..., 0.74161458,\n",
      "         0.61328713, 0.45062713]],\n",
      "\n",
      "       [[0.19349422, 0.25385389, 0.19126503, ..., 0.14452862,\n",
      "         0.07437871, 0.        ],\n",
      "        [0.26610782, 0.27336259, 0.21140692, ..., 0.16890535,\n",
      "         0.09829694, 0.01838778],\n",
      "        [0.23985873, 0.25530481, 0.12552362, ..., 0.15783717,\n",
      "         0.10319998, 0.02733009],\n",
      "        ...,\n",
      "        [0.94037908, 1.        , 0.80124571, ..., 0.71375387,\n",
      "         0.57085654, 0.40786499],\n",
      "        [0.87965003, 0.93483897, 0.76167426, ..., 0.73593255,\n",
      "         0.60478311, 0.43854613],\n",
      "        [0.74848377, 0.8840292 , 0.65129637, ..., 0.74431245,\n",
      "         0.6294722 , 0.46468342]],\n",
      "\n",
      "       [[0.25236039, 0.25975106, 0.19663483, ..., 0.15333711,\n",
      "         0.08140604, 0.        ],\n",
      "        [0.22561959, 0.24135502, 0.10914274, ..., 0.14206159,\n",
      "         0.08640092, 0.00910982],\n",
      "        [0.10066363, 0.20664575, 0.09798957, ..., 0.15546169,\n",
      "         0.10217954, 0.02349799],\n",
      "        ...,\n",
      "        [0.87739561, 0.93361836, 0.75720989, ..., 0.73098598,\n",
      "         0.59737982, 0.42802886],\n",
      "        [0.74377232, 0.88185681, 0.64476438, ..., 0.73952285,\n",
      "         0.62253139, 0.45465575],\n",
      "        [0.76360618, 0.81257278, 0.60240913, ..., 0.70880935,\n",
      "         0.62399204, 0.46884534]]])>, <tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
      "array([[1.        ],\n",
      "       [1.        ],\n",
      "       [1.        ],\n",
      "       [1.        ],\n",
      "       [0.93729773],\n",
      "       [0.88652459],\n",
      "       [0.81601915],\n",
      "       [0.75339366]])>)\n",
      "(<tf.Tensor: shape=(8, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.21850027, 0.23438036, 0.10095257, ..., 0.13417407,\n",
      "         0.07800168, 0.        ],\n",
      "        [0.09239551, 0.19935199, 0.08969687, ..., 0.14769736,\n",
      "         0.09392536, 0.01452045],\n",
      "        [0.16810722, 0.1975077 , 0.1371879 , ..., 0.15644246,\n",
      "         0.10690688, 0.02762323],\n",
      "        ...,\n",
      "        [0.74141667, 0.88077065, 0.6414985 , ..., 0.73712814,\n",
      "         0.61906111, 0.44964209],\n",
      "        [0.76143288, 0.81084965, 0.59875385, ..., 0.70613227,\n",
      "         0.62053518, 0.46396213],\n",
      "        [0.60173723, 0.75112647, 0.59305825, ..., 0.71249391,\n",
      "         0.63712947, 0.485117  ]],\n",
      "\n",
      "       [[0.07902251, 0.18755492, 0.0762841 , ..., 0.1351392 ,\n",
      "         0.0805749 , 0.        ],\n",
      "        [0.15584978, 0.18568346, 0.12447489, ..., 0.14401315,\n",
      "         0.09374769, 0.01329584],\n",
      "        [0.17726178, 0.23412193, 0.15084078, ..., 0.16398679,\n",
      "         0.11223587, 0.02934585],\n",
      "        ...,\n",
      "        [0.75791774, 0.80806264, 0.59284173, ..., 0.7018023 ,\n",
      "         0.61494399, 0.45606393],\n",
      "        [0.59586906, 0.74745947, 0.58706221, ..., 0.70825769,\n",
      "         0.63178279, 0.47753051],\n",
      "        [0.72549708, 0.81741997, 0.70738766, ..., 0.70926723,\n",
      "         0.64409176, 0.49627135]],\n",
      "\n",
      "       [[0.14447485, 0.17471054, 0.11267718, ..., 0.13247872,\n",
      "         0.08153593, 0.        ],\n",
      "        [0.16617538, 0.22380172, 0.13939836, ..., 0.15272151,\n",
      "         0.10027325, 0.01626628],\n",
      "        [0.20779133, 0.21125   , 0.15111333, ..., 0.16434356,\n",
      "         0.11460024, 0.03043687],\n",
      "        ...,\n",
      "        [0.5904234 , 0.74405648, 0.58149787, ..., 0.70432646,\n",
      "         0.62682106, 0.47049023],\n",
      "        [0.72179816, 0.8149597 , 0.70344471, ..., 0.7053496 ,\n",
      "         0.63929589, 0.48948361],\n",
      "        [0.73044483, 0.78042851, 0.57658872, ..., 0.67158912,\n",
      "         0.63127928, 0.49729995]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.15116078, 0.19386004, 0.07085088, ..., 0.14299537,\n",
      "         0.09000127, 0.        ],\n",
      "        [0.20006029, 0.30248015, 0.19883191, ..., 0.18712521,\n",
      "         0.12191643, 0.02379599],\n",
      "        [0.30879732, 0.32125611, 0.2672678 , ..., 0.22210591,\n",
      "         0.15078431, 0.04665692],\n",
      "        ...,\n",
      "        [0.52820165, 0.64600509, 0.47339446, ..., 0.62527213,\n",
      "         0.6035312 , 0.47902075],\n",
      "        [0.50942569, 0.53153575, 0.33412447, ..., 0.54394212,\n",
      "         0.56308288, 0.46794846],\n",
      "        [0.27955121, 0.38056731, 0.19520542, ..., 0.476925  ,\n",
      "         0.52405201, 0.45526316]],\n",
      "\n",
      "       [[0.18056093, 0.28547738, 0.17930261, ..., 0.16731054,\n",
      "         0.10051224, 0.        ],\n",
      "        [0.29194854, 0.30471102, 0.2494067 , ..., 0.20314393,\n",
      "         0.1300838 , 0.02341819],\n",
      "        [0.29212829, 0.2960829 , 0.24083846, ..., 0.22249063,\n",
      "         0.15174127, 0.04321333],\n",
      "        ...,\n",
      "        [0.49746743, 0.52011645, 0.31789306, ..., 0.53282524,\n",
      "         0.55243257, 0.45497915],\n",
      "        [0.26198952, 0.365468  , 0.17558772, ..., 0.4641745 ,\n",
      "         0.51245028, 0.44198463],\n",
      "        [0.31303972, 0.47092379, 0.30525035, ..., 0.44045793,\n",
      "         0.49225277, 0.43711918]],\n",
      "\n",
      "       [[0.27496964, 0.28803816, 0.23140765, ..., 0.18403552,\n",
      "         0.10922342, 0.        ],\n",
      "        [0.2751537 , 0.27920314, 0.22263395, ..., 0.20384614,\n",
      "         0.13140024, 0.02026982],\n",
      "        [0.26601179, 0.28319117, 0.21361478, ..., 0.20786427,\n",
      "         0.14470937, 0.036081  ],\n",
      "        ...,\n",
      "        [0.24429221, 0.35025208, 0.15581851, ..., 0.45132554,\n",
      "         0.50075896, 0.42860356],\n",
      "        [0.29656658, 0.45823667, 0.28859043, ..., 0.42704025,\n",
      "         0.48007712, 0.42362144],\n",
      "        [0.36957882, 0.52621777, 0.33282723, ..., 0.40320753,\n",
      "         0.4590846 , 0.41722178]]])>, <tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
      "array([[0.82007111],\n",
      "       [0.7833479 ],\n",
      "       [0.66238672],\n",
      "       [0.54582692],\n",
      "       [0.39068689],\n",
      "       [0.48351368],\n",
      "       [0.53731289],\n",
      "       [0.5051731 ]])>)\n",
      "(<tf.Tensor: shape=(2, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.26015722, 0.26429044, 0.20655088, 0.23792559, 0.18737437,\n",
      "         0.11342962, 0.        ],\n",
      "        [0.25082617, 0.26836098, 0.19734511, 0.20172879, 0.19147563,\n",
      "         0.12701411, 0.0161383 ],\n",
      "        [0.15451016, 0.24149522, 0.12419995, 0.21306376, 0.19764367,\n",
      "         0.14025251, 0.03189234],\n",
      "        [0.18983022, 0.19540378, 0.09545545, 0.12920991, 0.17809117,\n",
      "         0.13855365, 0.03967775],\n",
      "        [0.11430531, 0.16553199, 0.05487496, 0.13478346, 0.16571754,\n",
      "         0.13797362, 0.0472862 ],\n",
      "        [0.11875167, 0.20367013, 0.11474372, 0.19934904, 0.17532654,\n",
      "         0.14741599, 0.05945123],\n",
      "        [0.19227256, 0.27875663, 0.185697  , 0.25345638, 0.19764935,\n",
      "         0.1637299 , 0.07497164],\n",
      "        [0.26892457, 0.4402645 , 0.25621186, 0.43137187, 0.26442721,\n",
      "         0.20490559, 0.10348366],\n",
      "        [0.43650701, 0.79665908, 0.41007967, 0.69345426, 0.38700637,\n",
      "         0.28006692, 0.15068131],\n",
      "        [0.71831609, 0.85364718, 0.63245831, 0.73340856, 0.48597842,\n",
      "         0.34981179, 0.19729949],\n",
      "        [0.74355366, 0.90236883, 0.63383605, 0.89591854, 0.60310417,\n",
      "         0.43382821, 0.25318901],\n",
      "        [0.92923468, 1.        , 0.7640944 , 0.80310937, 0.66024851,\n",
      "         0.4906407 , 0.29718264],\n",
      "        [0.85715412, 0.92265902, 0.71712622, 0.7523837 , 0.68657285,\n",
      "         0.53090885, 0.33359873],\n",
      "        [0.70147016, 0.86235185, 0.58611641, 0.72138481, 0.69651913,\n",
      "         0.56021285, 0.36462161],\n",
      "        [0.72457852, 0.7816293 , 0.53676848, 0.5712745 , 0.66073495,\n",
      "         0.56191464, 0.38115384],\n",
      "        [0.54021274, 0.71267995, 0.53019301, 0.68644038, 0.66807936,\n",
      "         0.58107245, 0.40557677],\n",
      "        [0.68769294, 0.79227531, 0.66708951, 0.67209937, 0.66922793,\n",
      "         0.59507659, 0.42689858],\n",
      "        [0.69739962, 0.75351089, 0.52468205, 0.53658062, 0.6313287 ,\n",
      "         0.58607721, 0.43567314],\n",
      "        [0.49487285, 0.62099816, 0.43619396, 0.5174803 , 0.59880058,\n",
      "         0.57552384, 0.44221771],\n",
      "        [0.47477052, 0.49844248, 0.28708566, 0.29403696, 0.51172526,\n",
      "         0.53221816, 0.43036325],\n",
      "        [0.22865723, 0.33680932, 0.13835309, 0.26059554, 0.43997391,\n",
      "         0.49043007, 0.41678183],\n",
      "        [0.28201311, 0.44702803, 0.27387194, 0.35321685, 0.41518618,\n",
      "         0.46932034, 0.41169663],\n",
      "        [0.35653592, 0.5164156 , 0.31902397, 0.3300459 , 0.39086038,\n",
      "         0.4478935 , 0.40516458],\n",
      "        [0.32910647, 0.49493553, 0.27687788, 0.31082021, 0.36799176,\n",
      "         0.4268053 , 0.39761703]],\n",
      "\n",
      "       [[0.23853746, 0.25635989, 0.18417915, 0.18863473, 0.17821339,\n",
      "         0.1126945 , 0.        ],\n",
      "        [0.14064157, 0.22905345, 0.10983418, 0.20015563, 0.1844826 ,\n",
      "         0.12615006, 0.01601245],\n",
      "        [0.17654099, 0.18220597, 0.08061818, 0.11492632, 0.16460938,\n",
      "         0.12442333, 0.02392556],\n",
      "        [0.09977724, 0.15184419, 0.03937206, 0.1205913 , 0.15203279,\n",
      "         0.12383379, 0.03165882],\n",
      "        [0.10429654, 0.19060791, 0.10022285, 0.18621595, 0.1617994 ,\n",
      "         0.13343104, 0.04402339],\n",
      "        [0.1790234 , 0.26692606, 0.17233997, 0.24121081, 0.18448838,\n",
      "         0.15001254, 0.05979838],\n",
      "        [0.25693273, 0.43108315, 0.24401149, 0.42204465, 0.2523616 ,\n",
      "         0.19186364, 0.08877808],\n",
      "        [0.42726402, 0.79332368, 0.40040319, 0.68842599, 0.37695142,\n",
      "         0.26825785, 0.13674992],\n",
      "        [0.71369563, 0.85124655, 0.62642952, 0.72903565, 0.47754692,\n",
      "         0.33914674, 0.18413278],\n",
      "        [0.73934716, 0.90076739, 0.62782986, 0.89421129, 0.59659388,\n",
      "         0.42454129, 0.24093906],\n",
      "        [0.92807392, 1.        , 0.76022484, 0.79987977, 0.65467556,\n",
      "         0.48228567, 0.28565431],\n",
      "        [0.85481102, 0.9213904 , 0.71248623, 0.74832204, 0.6814317 ,\n",
      "         0.52321434, 0.32266773],\n",
      "        [0.69657337, 0.860094  , 0.57932747, 0.71681468, 0.69154112,\n",
      "         0.55299901, 0.35419949],\n",
      "        [0.72006077, 0.77804736, 0.52917008, 0.56424211, 0.65516998,\n",
      "         0.55472872, 0.3710029 ],\n",
      "        [0.53267084, 0.70796703, 0.52248676, 0.68129705, 0.66263486,\n",
      "         0.57420077, 0.39582643],\n",
      "        [0.68257016, 0.788868  , 0.66162877, 0.66672081, 0.66380227,\n",
      "         0.58843462, 0.41749798],\n",
      "        [0.69243606, 0.74946772, 0.5168854 , 0.52897914, 0.62528138,\n",
      "         0.57928762, 0.42641647],\n",
      "        [0.48658724, 0.61478139, 0.42694584, 0.50956552, 0.5922197 ,\n",
      "         0.56856115, 0.4330684 ],\n",
      "        [0.46615517, 0.49021542, 0.27539171, 0.28245703, 0.50371608,\n",
      "         0.52454513, 0.42101949],\n",
      "        [0.21600488, 0.32593099, 0.12421948, 0.24846707, 0.43078779,\n",
      "         0.48207158, 0.40721529],\n",
      "        [0.27023596, 0.43795762, 0.26196125, 0.34260765, 0.40559347,\n",
      "         0.46061559, 0.40204668],\n",
      "        [0.34598117, 0.50848336, 0.3078539 , 0.31905662, 0.38086865,\n",
      "         0.43883729, 0.39540748],\n",
      "        [0.31810179, 0.48665095, 0.26501649, 0.29951558, 0.35762492,\n",
      "         0.41740318, 0.38773613],\n",
      "        [0.26087919, 0.33611517, 0.12434671, 0.33229614, 0.35038812,\n",
      "         0.40430979, 0.38330093]]])>, <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
      "array([[0.34682915],\n",
      "       [0.364631  ]])>)\n"
     ]
    }
   ],
   "source": [
    "from training import GetTensoredDataset\n",
    "\n",
    "GetTensoredDataset = GetTensoredDataset()\n",
    "\n",
    "GetTensoredDataset.fit(window_size=25, batch_size=8, train=True, debug=False)\n",
    "\n",
    "x_train_tensors, _ = GetTensoredDataset.transform(x_train)\n",
    "c = 0\n",
    "\n",
    "for batch in x_train_tensors:\n",
    "    if c < 3:\n",
    "        print(batch)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------> GetTensoredDataset completed\n",
      "\n",
      "(<tf.Tensor: shape=(2, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.12665721, 0.21650782, 0.0953485 , 0.18713975, 0.17121167,\n",
      "         0.11192988, 0.        ],\n",
      "        [0.16314083, 0.16889799, 0.06565706, 0.1005235 , 0.15101505,\n",
      "         0.11017505, 0.00804188],\n",
      "        [0.0851279 , 0.13804213, 0.02373974, 0.10628066, 0.1382338 ,\n",
      "         0.10957591, 0.01590098],\n",
      "        [0.08972074, 0.17743666, 0.08558075, 0.17297322, 0.14815935,\n",
      "         0.11932935, 0.02846676],\n",
      "        [0.16566363, 0.25499673, 0.15887144, 0.22886302, 0.17121754,\n",
      "         0.13618068, 0.04449846],\n",
      "        [0.24484078, 0.42182515, 0.23170927, 0.41263957, 0.24019526,\n",
      "         0.17871282, 0.07394975],\n",
      "        [0.41794388, 0.78996043, 0.39064594, 0.68335574, 0.36681254,\n",
      "         0.25635019, 0.12270223],\n",
      "        [0.70903659, 0.84882588, 0.6203504 , 0.72462625, 0.46904503,\n",
      "         0.32839266, 0.17085615],\n",
      "        [0.73510555, 0.89915257, 0.62177353, 0.89248979, 0.59002925,\n",
      "         0.41517683, 0.22858684],\n",
      "        [0.92690346, 1.        , 0.75632297, 0.79662321, 0.64905609,\n",
      "         0.47386089, 0.27402975],\n",
      "        [0.85244835, 0.92011118, 0.70780752, 0.74422648, 0.67624763,\n",
      "         0.5154556 , 0.31164549],\n",
      "        [0.6916357 , 0.85781731, 0.57248186, 0.7122064 , 0.68652157,\n",
      "         0.54572495, 0.34369036],\n",
      "        [0.71550532, 0.77443552, 0.52150826, 0.55715101, 0.64955855,\n",
      "         0.54748281, 0.36076721],\n",
      "        [0.52506598, 0.70321478, 0.51471618, 0.67611079, 0.65714491,\n",
      "         0.56727173, 0.3859947 ],\n",
      "        [0.67740462, 0.78543224, 0.65612245, 0.66129735, 0.65833132,\n",
      "         0.58173721, 0.40801891],\n",
      "        [0.68743107, 0.7453908 , 0.50902366, 0.52131421, 0.61918357,\n",
      "         0.57244136, 0.41708254],\n",
      "        [0.47823246, 0.60851272, 0.41762052, 0.50158467, 0.58558389,\n",
      "         0.56154033, 0.42384271],\n",
      "        [0.4574679 , 0.48191968, 0.26360015, 0.27078044, 0.49564004,\n",
      "         0.51680804, 0.41159773],\n",
      "        [0.20324691, 0.31496186, 0.10996788, 0.23623736, 0.42152499,\n",
      "         0.47364332, 0.3975689 ],\n",
      "        [0.25836049, 0.42881149, 0.24995113, 0.33190989, 0.39592068,\n",
      "         0.45183818, 0.39231618],\n",
      "        [0.33533831, 0.5004849 , 0.2965906 , 0.30797562, 0.37079352,\n",
      "         0.42970548, 0.38556893],\n",
      "        [0.30700525, 0.47829721, 0.25305609, 0.28811658, 0.34717153,\n",
      "         0.40792257, 0.37777274],\n",
      "        [0.24885146, 0.32531176, 0.11009719, 0.32143058, 0.33981698,\n",
      "         0.39461611, 0.37326537],\n",
      "        [0.31075712, 0.35429163, 0.21307941, 0.24244738, 0.31199709,\n",
      "         0.37120554, 0.36279993]],\n",
      "\n",
      "       [[0.15635635, 0.16216018, 0.05808227, 0.09323137, 0.14413227,\n",
      "         0.10296117, 0.        ],\n",
      "        [0.07771096, 0.13105418, 0.01582512, 0.09903521, 0.13124739,\n",
      "         0.10235718, 0.00792282],\n",
      "        [0.08234103, 0.17076807, 0.07816749, 0.16626845, 0.14125341,\n",
      "         0.11218968, 0.02059047],\n",
      "        [0.1588996 , 0.24895693, 0.15205235, 0.22261135, 0.16449854,\n",
      "         0.12917763, 0.03675214],\n",
      "        [0.23871865, 0.41713785, 0.22548068, 0.4078778 , 0.23403547,\n",
      "         0.17205458, 0.06644219],\n",
      "        [0.4132251 , 0.78825762, 0.38570586, 0.68078868, 0.36167924,\n",
      "         0.25032136, 0.11558991],\n",
      "        [0.70667773, 0.8476003 , 0.61727255, 0.72239377, 0.46474054,\n",
      "         0.32294789, 0.16413422],\n",
      "        [0.73295803, 0.89833499, 0.61870722, 0.8916182 , 0.58670558,\n",
      "         0.41043563, 0.22233294],\n",
      "        [0.92631087, 1.        , 0.75434746, 0.79497442, 0.64621096,\n",
      "         0.46959544, 0.26814426],\n",
      "        [0.85125214, 0.91946352, 0.70543869, 0.74215291, 0.67362295,\n",
      "         0.51152736, 0.30606495],\n",
      "        [0.68913577, 0.85666462, 0.56901594, 0.70987324, 0.68398017,\n",
      "         0.54204211, 0.33836961],\n",
      "        [0.7131989 , 0.77260685, 0.51762909, 0.5535608 , 0.64671749,\n",
      "         0.54381422, 0.35558491],\n",
      "        [0.52121565, 0.70080872, 0.51078194, 0.673485  , 0.65436535,\n",
      "         0.56376357, 0.38101691],\n",
      "        [0.67478931, 0.78369272, 0.65333461, 0.65855147, 0.65556139,\n",
      "         0.57834632, 0.40321968],\n",
      "        [0.68489705, 0.74332667, 0.50504328, 0.51743346, 0.61609626,\n",
      "         0.56897511, 0.41235678],\n",
      "        [0.47400245, 0.6053389 , 0.41289913, 0.49754398, 0.58222418,\n",
      "         0.55798571, 0.41917176],\n",
      "        [0.45306955, 0.47771957, 0.2576301 , 0.2648686 , 0.49155116,\n",
      "         0.51289077, 0.4068275 ],\n",
      "        [0.19678758, 0.3094082 , 0.10275232, 0.23004548, 0.41683525,\n",
      "         0.46937611, 0.39268494],\n",
      "        [0.25234797, 0.42418082, 0.24387043, 0.32649363, 0.39102336,\n",
      "         0.44739419, 0.38738964],\n",
      "        [0.32994984, 0.49643529, 0.290888  , 0.30236532, 0.36569249,\n",
      "         0.42508206, 0.38058769],\n",
      "        [0.30138709, 0.47406773, 0.24700056, 0.28234529, 0.341879  ,\n",
      "         0.40312255, 0.3727283 ],\n",
      "        [0.24276184, 0.31984201, 0.10288268, 0.31592937, 0.33446482,\n",
      "         0.38970822, 0.36818438],\n",
      "        [0.30516938, 0.34905682, 0.20669978, 0.23630585, 0.3064194 ,\n",
      "         0.36610785, 0.3576341 ],\n",
      "        [0.22854564, 0.25482594, 0.08723191, 0.18178887, 0.27081068,\n",
      "         0.33775108, 0.34356648]]])>, <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
      "array([[0.26081854],\n",
      "       [0.47321994]])>)\n",
      "(<tf.Tensor: shape=(2, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.07034548, 0.1241147 , 0.00796542, 0.09184003, 0.12430946,\n",
      "         0.09518852, 0.        ],\n",
      "        [0.07501253, 0.16414575, 0.07080565, 0.1596102 , 0.13439538,\n",
      "         0.10509955, 0.01276882],\n",
      "        [0.15218249, 0.24295903, 0.14528056, 0.21640306, 0.15782615,\n",
      "         0.12222317, 0.02905955],\n",
      "        [0.23263899, 0.41248306, 0.2192953 , 0.40314906, 0.22791841,\n",
      "         0.16544253, 0.05898672],\n",
      "        [0.40853907, 0.78656663, 0.38080005, 0.67823943, 0.35658156,\n",
      "         0.24433436, 0.10852693],\n",
      "        [0.70433523, 0.84638323, 0.61421606, 0.72017678, 0.46046591,\n",
      "         0.31754089, 0.15745892],\n",
      "        [0.73082541, 0.89752309, 0.61566218, 0.89075265, 0.58340498,\n",
      "         0.40572731, 0.21612242],\n",
      "        [0.92572238, 1.        , 0.75238566, 0.79333707, 0.64338557,\n",
      "         0.46535958, 0.26229959],\n",
      "        [0.85006423, 0.91882035, 0.7030863 , 0.74009372, 0.67101647,\n",
      "         0.50762637, 0.30052312],\n",
      "        [0.68665318, 0.85551994, 0.56557406, 0.70755626, 0.68145641,\n",
      "         0.53838482, 0.33308577],\n",
      "        [0.71090848, 0.77079087, 0.51377683, 0.54999549, 0.64389615,\n",
      "         0.54017108, 0.35043855],\n",
      "        [0.51739204, 0.69841935, 0.506875  , 0.67087742, 0.65160508,\n",
      "         0.56027974, 0.37607366],\n",
      "        [0.67219215, 0.78196527, 0.65056611, 0.65582463, 0.65281067,\n",
      "         0.57497896, 0.39845374],\n",
      "        [0.68238061, 0.74127685, 0.50109051, 0.51357964, 0.61303038,\n",
      "         0.56553291, 0.40766381],\n",
      "        [0.46980179, 0.6021871 , 0.40821049, 0.49353132, 0.57888779,\n",
      "         0.55445574, 0.41453321],\n",
      "        [0.44870172, 0.47354859, 0.25170147, 0.25899778, 0.48749064,\n",
      "         0.50900067, 0.40209038],\n",
      "        [0.19037305, 0.30389307, 0.09558682, 0.22389655, 0.41217804,\n",
      "         0.4651385 , 0.38783487],\n",
      "        [0.24637715, 0.41958228, 0.23783191, 0.32111495, 0.38616002,\n",
      "         0.44298103, 0.38249728],\n",
      "        [0.32459876, 0.49241378, 0.28522497, 0.29679395, 0.36062686,\n",
      "         0.42049071, 0.37564101],\n",
      "        [0.2958079 , 0.46986758, 0.24098704, 0.27661403, 0.33662319,\n",
      "         0.39835584, 0.36771885],\n",
      "        [0.23671447, 0.31441021, 0.09571822, 0.31046632, 0.3291498 ,\n",
      "         0.38483437, 0.36313865],\n",
      "        [0.2996204 , 0.34385833, 0.20036441, 0.23020692, 0.3008804 ,\n",
      "         0.36104553, 0.35250411],\n",
      "        [0.22238474, 0.24887492, 0.07994246, 0.17525456, 0.26498731,\n",
      "         0.33246231, 0.33832415],\n",
      "        [0.16007037, 0.46901303, 0.11340027, 0.46138804, 0.3211018 ,\n",
      "         0.35229703, 0.34816926]],\n",
      "\n",
      "       [[0.06304877, 0.15333484, 0.05878748, 0.14874062, 0.12319968,\n",
      "         0.09352494, 0.        ],\n",
      "        [0.14121685, 0.23316749, 0.13422565, 0.20626804, 0.1469335 ,\n",
      "         0.11087003, 0.01650144],\n",
      "        [0.22271396, 0.40488413, 0.20919769, 0.39542941, 0.21793233,\n",
      "         0.1546484 , 0.04681568],\n",
      "        [0.40088913, 0.78380609, 0.37279134, 0.67407779, 0.3482596 ,\n",
      "         0.23456061, 0.09699665],\n",
      "        [0.70051111, 0.84439635, 0.60922634, 0.71655756, 0.45348759,\n",
      "         0.30871399, 0.14656152],\n",
      "        [0.72734392, 0.89619765, 0.61069117, 0.88933965, 0.57801675,\n",
      "         0.39804101, 0.20598377],\n",
      "        [0.92476167, 1.        , 0.74918302, 0.79066409, 0.63877313,\n",
      "         0.45844456, 0.2527582 ],\n",
      "        [0.84812496, 0.91777037, 0.69924603, 0.7367321 , 0.66676141,\n",
      "         0.50125803, 0.29147611],\n",
      "        [0.68260036, 0.85365124, 0.55995521, 0.7037738 , 0.67733638,\n",
      "         0.5324143 , 0.32445992],\n",
      "        [0.70716938, 0.76782629, 0.50748804, 0.54417515, 0.63929031,\n",
      "         0.53422366, 0.34203714],\n",
      "        [0.51115   , 0.69451871, 0.50049694, 0.66662056, 0.64709896,\n",
      "         0.55459242, 0.36800382],\n",
      "        [0.6679523 , 0.77914522, 0.64604654, 0.65137307, 0.64832013,\n",
      "         0.56948175, 0.39067336],\n",
      "        [0.67827253, 0.73793054, 0.49463763, 0.5072883 , 0.60802532,\n",
      "         0.55991353, 0.40000255],\n",
      "        [0.46294422, 0.5970418 , 0.4005563 , 0.48698067, 0.57344114,\n",
      "         0.54869309, 0.4069608 ],\n",
      "        [0.44157124, 0.46673948, 0.242023  , 0.24941368, 0.48086186,\n",
      "         0.5026501 , 0.39435703],\n",
      "        [0.17990136, 0.29488965, 0.08388917, 0.21385846, 0.40457518,\n",
      "         0.45822062, 0.37991715],\n",
      "        [0.23662982, 0.41207517, 0.22797405, 0.31233427, 0.37822063,\n",
      "         0.43577656, 0.37451052],\n",
      "        [0.31586315, 0.48584867, 0.27598009, 0.28769871, 0.35235722,\n",
      "         0.41299536, 0.36756557],\n",
      "        [0.28669991, 0.46301087, 0.23116999, 0.26725778, 0.3280431 ,\n",
      "         0.39057419, 0.35954095],\n",
      "        [0.22684216, 0.30554281, 0.08402227, 0.30154791, 0.32047305,\n",
      "         0.37687784, 0.35490151],\n",
      "        [0.29056171, 0.33537182, 0.19002195, 0.22025044, 0.29183801,\n",
      "         0.35278132, 0.34412942],\n",
      "        [0.21232709, 0.23915989, 0.06804247, 0.16458733, 0.25548068,\n",
      "         0.3238284 , 0.32976605],\n",
      "        [0.14920675, 0.46214526, 0.10193302, 0.45442165, 0.31232095,\n",
      "         0.34391966, 0.3397385 ],\n",
      "        [0.45801717, 0.58126154, 0.39596208, 0.53691767, 0.37649144,\n",
      "         0.37361167, 0.35551283]]])>, <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
      "array([[0.58660833],\n",
      "       [0.62041218]])>)\n",
      "(<tf.Tensor: shape=(2, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.12680792, 0.22030134, 0.11969942, 0.19295056, 0.13262048,\n",
      "         0.09595193, 0.        ],\n",
      "        [0.20967242, 0.39489909, 0.19592937, 0.38528573, 0.20481056,\n",
      "         0.14046482, 0.03082286],\n",
      "        [0.39083706, 0.78017872, 0.36226784, 0.66860936, 0.3373245 ,\n",
      "         0.22171783, 0.08184578],\n",
      "        [0.69548619, 0.84178558, 0.60266982, 0.71180187, 0.44431804,\n",
      "         0.29711538, 0.13224227],\n",
      "        [0.72276921, 0.89445603, 0.60415922, 0.88748295, 0.57093658,\n",
      "         0.38794116, 0.19266152],\n",
      "        [0.9234993 , 1.        , 0.74497474, 0.78715179, 0.63271236,\n",
      "         0.44935818, 0.24022074],\n",
      "        [0.84557676, 0.9163907 , 0.69419988, 0.73231491, 0.66117023,\n",
      "         0.49288998, 0.27958828],\n",
      "        [0.67727493, 0.85119575, 0.552572  , 0.69880363, 0.67192263,\n",
      "         0.524569  , 0.3131255 ],\n",
      "        [0.70225618, 0.76393081, 0.49922452, 0.53652718, 0.63323821,\n",
      "         0.52640872, 0.33099764],\n",
      "        [0.50294793, 0.68939325, 0.49211612, 0.66102702, 0.64117787,\n",
      "         0.54711923, 0.35739999],\n",
      "        [0.6623811 , 0.77543965, 0.6401078 , 0.6455237 , 0.64241954,\n",
      "         0.56225838, 0.38044989],\n",
      "        [0.67287448, 0.73353345, 0.48615851, 0.49902143, 0.60144865,\n",
      "         0.55252962, 0.38993561],\n",
      "        [0.45393333, 0.59028084, 0.39049865, 0.47837307, 0.5662842 ,\n",
      "         0.54112092, 0.39701061],\n",
      "        [0.43220175, 0.45779227, 0.22930542, 0.23682011, 0.4721516 ,\n",
      "         0.49430541, 0.38419537],\n",
      "        [0.16614149, 0.28305909, 0.06851838, 0.20066833, 0.39458495,\n",
      "         0.44913047, 0.3695132 ],\n",
      "        [0.22382176, 0.40221079, 0.21502076, 0.3007964 , 0.36778822,\n",
      "         0.42630985, 0.36401586],\n",
      "        [0.30438449, 0.47722208, 0.26383226, 0.2757475 , 0.34149087,\n",
      "         0.40314641, 0.35695439],\n",
      "        [0.27473194, 0.4540011 , 0.21827032, 0.25496361, 0.3167688 ,\n",
      "         0.38034906, 0.34879513],\n",
      "        [0.21386988, 0.29389099, 0.06865371, 0.28982907, 0.30907173,\n",
      "         0.3664229 , 0.34407784],\n",
      "        [0.27865854, 0.32422048, 0.17643189, 0.20716756, 0.27995625,\n",
      "         0.34192208, 0.33312502],\n",
      "        [0.19911127, 0.22639428, 0.0524058 , 0.15057052, 0.2429889 ,\n",
      "         0.31248338, 0.31852066],\n",
      "        [0.13493188, 0.45312097, 0.08686497, 0.44526777, 0.30078286,\n",
      "         0.33291175, 0.32866043],\n",
      "        [0.44892361, 0.57423582, 0.38582734, 0.52914793, 0.36603002,\n",
      "         0.36310193, 0.34469943],\n",
      "        [0.53348061, 0.61404334, 0.4326757 , 0.46280199, 0.39367916,\n",
      "         0.3784404 , 0.35414763]],\n",
      "\n",
      "       [[0.18453753, 0.375655  , 0.17035741, 0.3657359 , 0.17952105,\n",
      "         0.11312892, 0.        ],\n",
      "        [0.37146378, 0.77318772, 0.34198597, 0.65807011, 0.31624935,\n",
      "         0.19696603, 0.05264561],\n",
      "        [0.68580171, 0.83675387, 0.59003348, 0.70263627, 0.42664561,\n",
      "         0.27476145, 0.10464486],\n",
      "        [0.7139524 , 0.8910994 , 0.59157025, 0.88390456, 0.55729103,\n",
      "         0.36847577, 0.16698564],\n",
      "        [0.92106634, 1.        , 0.73686414, 0.78038255, 0.62103146,\n",
      "         0.43184605, 0.21605739],\n",
      "        [0.84066562, 0.91373166, 0.68447448, 0.72380168, 0.65039438,\n",
      "         0.4767623 , 0.25667693],\n",
      "        [0.66701127, 0.84646331, 0.53834239, 0.68922464, 0.66148874,\n",
      "         0.50944881, 0.29128075],\n",
      "        [0.69278699, 0.75642307, 0.4832983 , 0.5217873 , 0.62157404,\n",
      "         0.51134704, 0.30972127],\n",
      "        [0.48714012, 0.67951499, 0.47596383, 0.65024662, 0.62976621,\n",
      "         0.53271621, 0.3369633 ],\n",
      "        [0.65164376, 0.76829793, 0.62866211, 0.63425025, 0.63104736,\n",
      "         0.54833683, 0.36074626],\n",
      "        [0.66247087, 0.72505899, 0.46981674, 0.48308875, 0.58877347,\n",
      "         0.53829866, 0.37053366],\n",
      "        [0.43656671, 0.5772505 , 0.3711146 , 0.46178371, 0.55249068,\n",
      "         0.52652713, 0.37783366],\n",
      "        [0.414144  , 0.44054837, 0.20479493, 0.2125486 , 0.45536437,\n",
      "         0.47822274, 0.36461086],\n",
      "        [0.13962219, 0.26025813, 0.03889436, 0.17524709, 0.37533086,\n",
      "         0.4316111 , 0.34946176],\n",
      "        [0.19913687, 0.38319923, 0.19005597, 0.27855955, 0.34768192,\n",
      "         0.40806471, 0.34378958],\n",
      "        [0.28226174, 0.46059611, 0.24041983, 0.25271401, 0.32054823,\n",
      "         0.3841646 , 0.33650353],\n",
      "        [0.25166615, 0.43663663, 0.19340888, 0.23126912, 0.29503991,\n",
      "         0.36064222, 0.32808478],\n",
      "        [0.18886848, 0.27143452, 0.03903399, 0.26724342, 0.28709806,\n",
      "         0.34627317, 0.32321747],\n",
      "        [0.25571763, 0.30272858, 0.15023986, 0.18195301, 0.25705661,\n",
      "         0.32099315, 0.31191631],\n",
      "        [0.1736405 , 0.2017912 , 0.02226934, 0.12355601, 0.21891358,\n",
      "         0.2906182 , 0.29684749],\n",
      "        [0.10742001, 0.43572851, 0.05782443, 0.42762555, 0.27854557,\n",
      "         0.31169626, 0.30730973],\n",
      "        [0.43139766, 0.56069519, 0.36629474, 0.51417336, 0.3458678 ,\n",
      "         0.34284658, 0.32385882],\n",
      "        [0.51864383, 0.60176871, 0.41463302, 0.44571742, 0.37439626,\n",
      "         0.35867286, 0.33360751],\n",
      "        [0.44683509, 0.50481289, 0.19075444, 0.1947361 , 0.32306479,\n",
      "         0.33345182, 0.3224978 ]]])>, <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
      "array([[0.52007597],\n",
      "       [0.17084643]])>)\n",
      "(<tf.Tensor: shape=(2, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.40595491, 0.78563411, 0.3780947 , 0.67683362, 0.35377039,\n",
      "         0.24103277, 0.10463198],\n",
      "        [0.70304344, 0.84571206, 0.61253052, 0.7189542 , 0.45810862,\n",
      "         0.31455915, 0.15377776],\n",
      "        [0.72964936, 0.89707535, 0.61398296, 0.89027534, 0.58158482,\n",
      "         0.40313087, 0.21269757],\n",
      "        [0.92539785, 1.        , 0.7513038 , 0.79243413, 0.64182748,\n",
      "         0.46302368, 0.25907649],\n",
      "        [0.84940914, 0.91846566, 0.70178905, 0.73895816, 0.6695791 ,\n",
      "         0.50547514, 0.29746702],\n",
      "        [0.68528413, 0.85488869, 0.563676  , 0.70627854, 0.68006466,\n",
      "         0.53636797, 0.33017195],\n",
      "        [0.7096454 , 0.76978943, 0.51165247, 0.54802937, 0.64234029,\n",
      "         0.53816203, 0.34760054],\n",
      "        [0.51528347, 0.69710171, 0.50472048, 0.66943945, 0.65008291,\n",
      "         0.55835856, 0.37334765],\n",
      "        [0.67075992, 0.78101265, 0.64903939, 0.65432089, 0.65129376,\n",
      "         0.57312199, 0.39582551],\n",
      "        [0.68099289, 0.74014646, 0.49891072, 0.51145442, 0.61133966,\n",
      "         0.56363467, 0.40507582],\n",
      "        [0.46748529, 0.60044901, 0.40562489, 0.4913185 , 0.5770479 ,\n",
      "         0.55250911, 0.41197524],\n",
      "        [0.44629303, 0.47124846, 0.24843206, 0.25576025, 0.48525143,\n",
      "         0.50685544, 0.39947804],\n",
      "        [0.1868357 , 0.3008517 , 0.09163534, 0.22050567, 0.40960978,\n",
      "         0.46280163, 0.38516025],\n",
      "        [0.24308449, 0.41704637, 0.23450191, 0.31814882, 0.38347808,\n",
      "         0.44054735, 0.37979933],\n",
      "        [0.32164786, 0.49019607, 0.28210204, 0.29372156, 0.35783336,\n",
      "         0.41795877, 0.37291311],\n",
      "        [0.29273121, 0.46755137, 0.23767082, 0.27345347, 0.33372482,\n",
      "         0.39572718, 0.36495634],\n",
      "        [0.23337959, 0.31141479, 0.09176731, 0.30745367, 0.32621878,\n",
      "         0.38214664, 0.36035613],\n",
      "        [0.29656036, 0.34099157, 0.19687071, 0.2268436 , 0.29782587,\n",
      "         0.35825387, 0.34967513],\n",
      "        [0.21898724, 0.24559316, 0.07592262, 0.17165115, 0.26177595,\n",
      "         0.32954576, 0.33543321],\n",
      "        [0.15640062, 0.46669309, 0.10952661, 0.45903478, 0.31813562,\n",
      "         0.34946714, 0.34532133],\n",
      "        [0.4625999 , 0.58480218, 0.40106952, 0.54083325, 0.38176351,\n",
      "         0.37890808, 0.36096229],\n",
      "        [0.54505841, 0.62362178, 0.44675522, 0.47613385, 0.40872647,\n",
      "         0.39386589, 0.37017601],\n",
      "        [0.47719019, 0.53198644, 0.23516204, 0.23892521, 0.36021182,\n",
      "         0.37002887, 0.35967595],\n",
      "        [0.17917739, 0.2163465 , 0.10979075, 0.1160626 , 0.2904549 ,\n",
      "         0.33095713, 0.34018688]],\n",
      "\n",
      "       [[0.72501064, 0.85712543, 0.64119337, 0.73974442, 0.49819475,\n",
      "         0.36526428, 0.21637661],\n",
      "        [0.74964841, 0.90468915, 0.64253837, 0.89839216, 0.61253687,\n",
      "         0.44728395, 0.27093785],\n",
      "        [0.9309165 , 1.        , 0.76970097, 0.80778871, 0.66832311,\n",
      "         0.50274622, 0.31388592],\n",
      "        [0.86054902, 0.92449712, 0.72384905, 0.75826859, 0.69402182,\n",
      "         0.54205736, 0.34943654],\n",
      "        [0.70856507, 0.86562322, 0.59595284, 0.72800643, 0.70373171,\n",
      "         0.57066491, 0.37972213],\n",
      "        [0.73112423, 0.78681914, 0.54777772, 0.58146366, 0.66879798,\n",
      "         0.57232625, 0.39586145],\n",
      "        [0.55114012, 0.71950845, 0.54135853, 0.69389249, 0.67596784,\n",
      "         0.59102875, 0.41970393],\n",
      "        [0.69511528, 0.79721213, 0.67500152, 0.67989232, 0.67708912,\n",
      "         0.60470007, 0.440519  ],\n",
      "        [0.70459127, 0.75936899, 0.53597854, 0.54759432, 0.64009061,\n",
      "         0.59591457, 0.44908503],\n",
      "        [0.50687779, 0.63000559, 0.44959348, 0.52894795, 0.60833556,\n",
      "         0.58561201, 0.45547406],\n",
      "        [0.48725322, 0.51036258, 0.30402891, 0.310815  , 0.52332969,\n",
      "         0.54333555, 0.44390134],\n",
      "        [0.2469891 , 0.35257083, 0.15883114, 0.27816836, 0.45328359,\n",
      "         0.5025406 , 0.4306427 ],\n",
      "        [0.29907692, 0.46017006, 0.29112923, 0.36858841, 0.42908497,\n",
      "         0.48193257, 0.42567836],\n",
      "        [0.3718286 , 0.52790855, 0.33520816, 0.34596814, 0.4053373 ,\n",
      "         0.46101496, 0.41930154],\n",
      "        [0.34505105, 0.50693898, 0.29406373, 0.32719938, 0.38301218,\n",
      "         0.44042795, 0.41193337],\n",
      "        [0.29008993, 0.36235252, 0.15895335, 0.35868442, 0.37606139,\n",
      "         0.42785202, 0.40767345],\n",
      "        [0.34859694, 0.38974138, 0.25628178, 0.28403744, 0.34976884,\n",
      "         0.4057267 , 0.39778257],\n",
      "        [0.27676226, 0.30140002, 0.14428077, 0.23292783, 0.31638569,\n",
      "         0.37914226, 0.38459419],\n",
      "        [0.21880545, 0.50614419, 0.17539892, 0.4990524 , 0.36857618,\n",
      "         0.39758997, 0.39375085],\n",
      "        [0.50235379, 0.61551622, 0.44537509, 0.57479987, 0.42749723,\n",
      "         0.42485304, 0.40823477],\n",
      "        [0.57871247, 0.65146416, 0.48768121, 0.51488658, 0.45246562,\n",
      "         0.43870435, 0.41676691],\n",
      "        [0.51586477, 0.5666075 , 0.29174053, 0.29522532, 0.40753982,\n",
      "         0.41663065, 0.40704359],\n",
      "        [0.23989731, 0.27431686, 0.17564351, 0.18145141, 0.34294313,\n",
      "         0.38044923, 0.38899621],\n",
      "        [0.17790551, 0.26777527, 0.07397448, 0.07489148, 0.26635694,\n",
      "         0.33344035, 0.36386783]]])>, <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
      "array([[0.20928234],\n",
      "       [0.11236782]])>)\n",
      "(<tf.Tensor: shape=(2, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.75415735, 0.90640574, 0.64897641, 0.90022216, 0.61951525,\n",
      "         0.45723861, 0.28406859],\n",
      "        [0.93216073, 1.        , 0.77384876, 0.81125052, 0.67429676,\n",
      "         0.51170198, 0.32624314],\n",
      "        [0.8630606 , 0.92585696, 0.72882265, 0.76262228, 0.69953262,\n",
      "         0.55030511, 0.36115347],\n",
      "        [0.71381394, 0.86804341, 0.60322991, 0.73290516, 0.70906763,\n",
      "         0.57839742, 0.39089361],\n",
      "        [0.73596681, 0.79065862, 0.55592245, 0.58900169, 0.67476308,\n",
      "         0.58002885, 0.40674225],\n",
      "        [0.55922429, 0.72456023, 0.54961886, 0.69940562, 0.6818038 ,\n",
      "         0.59839451, 0.43015532],\n",
      "        [0.70060639, 0.80086443, 0.68085489, 0.6856576 , 0.68290489,\n",
      "         0.6118196 , 0.45059551],\n",
      "        [0.70991172, 0.76370286, 0.54433577, 0.55574235, 0.64657273,\n",
      "         0.60319233, 0.45900725],\n",
      "        [0.51575914, 0.63666935, 0.45950654, 0.5374318 , 0.61538961,\n",
      "         0.59307532, 0.46528122],\n",
      "        [0.49648801, 0.51918117, 0.31656366, 0.32322753, 0.53191473,\n",
      "         0.55156028, 0.45391692],\n",
      "        [0.26055116, 0.36423132, 0.17398097, 0.29116887, 0.4631302 ,\n",
      "         0.51150006, 0.44089708],\n",
      "        [0.31170085, 0.46989263, 0.30389631, 0.37996041, 0.4393674 ,\n",
      "         0.49126319, 0.43602214],\n",
      "        [0.38314225, 0.53641113, 0.34718136, 0.35774755, 0.41604744,\n",
      "         0.47072232, 0.42976018],\n",
      "        [0.35684697, 0.51581923, 0.30677795, 0.33931681, 0.39412441,\n",
      "         0.45050609, 0.42252471],\n",
      "        [0.30287573, 0.37383683, 0.17410097, 0.3702348 , 0.3872988 ,\n",
      "         0.43815666, 0.41834152],\n",
      "        [0.360329  , 0.40073241, 0.26967648, 0.29693225, 0.36147979,\n",
      "         0.41642983, 0.40862877],\n",
      "        [0.28978809, 0.31398211, 0.15969265, 0.24674314, 0.32869789,\n",
      "         0.39032418, 0.39567792],\n",
      "        [0.23287511, 0.51503875, 0.19025035, 0.50807469, 0.3799484 ,\n",
      "         0.40843965, 0.40466966],\n",
      "        [0.51131662, 0.62244094, 0.45536413, 0.58245792, 0.43780826,\n",
      "         0.43521169, 0.41889272],\n",
      "        [0.58630005, 0.65774145, 0.4969083 , 0.52362369, 0.46232696,\n",
      "         0.44881353, 0.4272712 ],\n",
      "        [0.52458427, 0.57441309, 0.3044966 , 0.30791863, 0.41821029,\n",
      "         0.4271374 , 0.417723  ],\n",
      "        [0.2535871 , 0.28738673, 0.19049054, 0.19619383, 0.35477702,\n",
      "         0.39160762, 0.40000066],\n",
      "        [0.1927118 , 0.28096296, 0.09065261, 0.0915531 , 0.27957018,\n",
      "         0.34544538, 0.37532486],\n",
      "        [0.07276226, 0.12835447, 0.01801045, 0.03343944, 0.20924711,\n",
      "         0.29744447, 0.34797402]],\n",
      "\n",
      "       [[0.94077575, 1.        , 0.80256807, 0.83522012, 0.71565833,\n",
      "         0.57371172, 0.4118046 ],\n",
      "        [0.88045075, 0.93527251, 0.76325989, 0.79276725, 0.73768945,\n",
      "         0.60741258, 0.44228161],\n",
      "        [0.75015717, 0.88480078, 0.65361638, 0.76682395, 0.74601359,\n",
      "         0.6319374 , 0.468245  ],\n",
      "        [0.7694968 , 0.81724322, 0.61231656, 0.64119502, 0.71606543,\n",
      "         0.63336165, 0.482081  ],\n",
      "        [0.6151991 , 0.75953876, 0.60681348, 0.73757858, 0.72221205,\n",
      "         0.64939503, 0.50252081],\n",
      "        [0.73862686, 0.82615297, 0.72138363, 0.72557644, 0.7231733 ,\n",
      "         0.66111524, 0.52036526],\n",
      "        [0.74675049, 0.79371061, 0.6022013 , 0.61215934, 0.69145503,\n",
      "         0.65358357, 0.52770878],\n",
      "        [0.57725366, 0.68280929, 0.52814467, 0.59617407, 0.6642319 ,\n",
      "         0.64475134, 0.53318601],\n",
      "        [0.5604298 , 0.58024112, 0.40335433, 0.40917194, 0.59135762,\n",
      "         0.60850835, 0.52326488],\n",
      "        [0.35445495, 0.44496859, 0.27887844, 0.38118446, 0.53130815,\n",
      "         0.57353545, 0.51189845],\n",
      "        [0.39910906, 0.53721181, 0.39229562, 0.45870022, 0.51056303,\n",
      "         0.55586849, 0.50764259],\n",
      "        [0.461478  , 0.59528302, 0.43008384, 0.43930821, 0.49020451,\n",
      "         0.53793614, 0.50217584],\n",
      "        [0.438522  , 0.57730612, 0.39481132, 0.42321802, 0.47106551,\n",
      "         0.5202872 , 0.49585921],\n",
      "        [0.39140465, 0.45335429, 0.2789832 , 0.45020968, 0.4651067 ,\n",
      "         0.50950604, 0.49220725],\n",
      "        [0.44156184, 0.47683435, 0.36242143, 0.38621594, 0.44256649,\n",
      "         0.49053833, 0.48372795],\n",
      "        [0.37997903, 0.40110062, 0.26640462, 0.34240042, 0.41394761,\n",
      "         0.46774789, 0.47242174],\n",
      "        [0.33029352, 0.57662475, 0.29308175, 0.57054507, 0.45868974,\n",
      "         0.48356284, 0.48027161],\n",
      "        [0.5733753 , 0.67038777, 0.5245283 , 0.63548225, 0.50920189,\n",
      "         0.50693505, 0.49268846],\n",
      "        [0.63883646, 0.7012054 , 0.56079672, 0.58411947, 0.53060691,\n",
      "         0.51880958, 0.50000294],\n",
      "        [0.58495807, 0.62845905, 0.39281968, 0.39580714, 0.49209269,\n",
      "         0.49988613, 0.49166728],\n",
      "        [0.34837526, 0.37788263, 0.29329144, 0.29827045, 0.43671491,\n",
      "         0.46886833, 0.47619553],\n",
      "        [0.29523061, 0.37227462, 0.20613208, 0.20691821, 0.37105871,\n",
      "         0.42856831, 0.45465335],\n",
      "        [0.19051366, 0.23904612, 0.14271486, 0.1561845 , 0.30966608,\n",
      "         0.38666311, 0.43077584],\n",
      "        [0.15587005, 0.25330189, 0.12699159, 0.16289309, 0.26773094,\n",
      "         0.35223695, 0.40934522]]])>, <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
      "array([[0.14468395],\n",
      "       [0.14088053]])>)\n",
      "(<tf.Tensor: shape=(2, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.94138419, 1.        , 0.81608289, 0.84763237, 0.7887428 ,\n",
      "         0.64944984, 0.47289064],\n",
      "        [0.80207337, 0.94603528, 0.69885127, 0.81989361, 0.79764303,\n",
      "         0.67567196, 0.50065087],\n",
      "        [0.82275144, 0.87380224, 0.65469321, 0.68557026, 0.76562224,\n",
      "         0.67719477, 0.51544442],\n",
      "        [0.65777524, 0.81210423, 0.64880928, 0.78862425, 0.77219424,\n",
      "         0.69433777, 0.53729881],\n",
      "        [0.78974508, 0.88332862, 0.7713085 , 0.77579148, 0.77322203,\n",
      "         0.70686911, 0.55637822],\n",
      "        [0.79843092, 0.84864102, 0.6438779 , 0.65452511, 0.73930862,\n",
      "         0.69881619, 0.56422998],\n",
      "        [0.61720371, 0.73006453, 0.56469603, 0.63743355, 0.71020146,\n",
      "         0.6893727 , 0.57008626],\n",
      "        [0.59921552, 0.62039792, 0.43126931, 0.43748954, 0.63228377,\n",
      "         0.65062145, 0.55947852],\n",
      "        [0.37898574, 0.47576358, 0.2981788 , 0.40756513, 0.56807844,\n",
      "         0.61322817, 0.54732545],\n",
      "        [0.42673024, 0.57439068, 0.41944526, 0.49044554, 0.54589761,\n",
      "         0.59433853, 0.54277506],\n",
      "        [0.49341555, 0.63648083, 0.45984869, 0.46971145, 0.52413014,\n",
      "         0.57516514, 0.53692997],\n",
      "        [0.46887083, 0.6172598 , 0.42213507, 0.45250771, 0.50366659,\n",
      "         0.55629476, 0.53017619],\n",
      "        [0.41849263, 0.48472962, 0.29829082, 0.48136739, 0.49729539,\n",
      "         0.54476747, 0.52627149],\n",
      "        [0.47212105, 0.50983467, 0.38750356, 0.41294482, 0.47319523,\n",
      "         0.52448707, 0.51720535],\n",
      "        [0.40627627, 0.42885963, 0.28484171, 0.36609696, 0.44259572,\n",
      "         0.50011936, 0.50511668],\n",
      "        [0.35315217, 0.61653128, 0.31336508, 0.61003083, 0.49043433,\n",
      "         0.51702882, 0.51350981],\n",
      "        [0.61305694, 0.71678336, 0.56082938, 0.67946213, 0.54444227,\n",
      "         0.54201856, 0.526786  ],\n",
      "        [0.68304848, 0.74973379, 0.59960783, 0.62454468, 0.56732867,\n",
      "         0.55471488, 0.53460669],\n",
      "        [0.62544131, 0.67195287, 0.42000559, 0.42319981, 0.526149  ,\n",
      "         0.5344818 , 0.52569414],\n",
      "        [0.3724853 , 0.40403478, 0.31358928, 0.31891289, 0.46693868,\n",
      "         0.50131735, 0.50915164],\n",
      "        [0.31566267, 0.39803867, 0.22039788, 0.22123842, 0.39673861,\n",
      "         0.45822828, 0.48611858],\n",
      "        [0.20369855, 0.25558981, 0.15259174, 0.16699358, 0.33109717,\n",
      "         0.41342294, 0.46058858],\n",
      "        [0.16665736, 0.27083218, 0.13578031, 0.17416645, 0.28625982,\n",
      "         0.37661425, 0.43767481],\n",
      "        [0.138246  , 0.15063046, 0.        , 0.11986551, 0.23871859,\n",
      "         0.33711444, 0.41225007]],\n",
      "\n",
      "       [[0.84782607, 1.        , 0.73871587, 0.86666283, 0.84314301,\n",
      "         0.71421434, 0.52920952],\n",
      "        [0.86968368, 0.92364658, 0.6920389 , 0.72467727, 0.80929566,\n",
      "         0.71582402, 0.54484694],\n",
      "        [0.69529674, 0.85842913, 0.68581933, 0.83360978, 0.81624255,\n",
      "         0.7339449 , 0.56794797],\n",
      "        [0.83479454, 0.93371637, 0.81530628, 0.82004498, 0.81732896,\n",
      "         0.74719107, 0.58811573],\n",
      "        [0.84397585, 0.89705008, 0.68060665, 0.6918612 , 0.78148103,\n",
      "         0.73867878, 0.59641537],\n",
      "        [0.65241088, 0.77170963, 0.596908  , 0.6737947 , 0.7507135 ,\n",
      "         0.72869662, 0.60260571],\n",
      "        [0.63339659, 0.6557873 , 0.45587022, 0.46244527, 0.66835115,\n",
      "         0.68773487, 0.59139288],\n",
      "        [0.40060424, 0.50290258, 0.31518783, 0.43081388, 0.60048336,\n",
      "         0.64820856, 0.57854656],\n",
      "        [0.45107222, 0.60715567, 0.4433717 , 0.51842204, 0.57703727,\n",
      "         0.62824141, 0.5737366 ],\n",
      "        [0.52156147, 0.67278763, 0.48607986, 0.49650522, 0.55402811,\n",
      "         0.6079743 , 0.56755809],\n",
      "        [0.49561665, 0.65247017, 0.44621494, 0.47832013, 0.53239726,\n",
      "         0.5880275 , 0.56041905],\n",
      "        [0.44236472, 0.51238007, 0.31530623, 0.50882605, 0.52566263,\n",
      "         0.57584267, 0.55629161],\n",
      "        [0.49905227, 0.53891719, 0.40960794, 0.43650045, 0.50018772,\n",
      "         0.5544054 , 0.54670832],\n",
      "        [0.42945151, 0.45332308, 0.30108994, 0.38698024, 0.46784273,\n",
      "         0.52864768, 0.53393007],\n",
      "        [0.37329705, 0.65170009, 0.33124038, 0.64482885, 0.51841019,\n",
      "         0.54652171, 0.54280197],\n",
      "        [0.64802757, 0.75767086, 0.59282079, 0.71822071, 0.57549891,\n",
      "         0.57293694, 0.55683547],\n",
      "        [0.72201164, 0.79250089, 0.63381128, 0.66017061, 0.59969082,\n",
      "         0.58635751, 0.56510228],\n",
      "        [0.66111839, 0.71028311, 0.44396399, 0.44734041, 0.55616213,\n",
      "         0.56497026, 0.55568133],\n",
      "        [0.393733  , 0.42708216, 0.33147737, 0.33710465, 0.49357428,\n",
      "         0.52991401, 0.5381952 ],\n",
      "        [0.33366902, 0.420744  , 0.23297005, 0.23385853, 0.41936978,\n",
      "         0.48436701, 0.51384826],\n",
      "        [0.21531814, 0.27016943, 0.16129604, 0.1765194 , 0.34998396,\n",
      "         0.43700584, 0.48686196],\n",
      "        [0.176164  , 0.28628127, 0.14352563, 0.18410143, 0.30258895,\n",
      "         0.39809747, 0.46264111],\n",
      "        [0.14613197, 0.15922288, 0.        , 0.126703  , 0.25233582,\n",
      "         0.35634448, 0.43576606],\n",
      "        [0.11503376, 0.16609412, 0.01113615, 0.06675752, 0.19931345,\n",
      "         0.31179264, 0.40624538]]])>, <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
      "array([[0.1571309 ],\n",
      "       [0.19630376]])>)\n",
      "(<tf.Tensor: shape=(2, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.93142169, 0.98921537, 0.74116608, 0.77612142, 0.86674678,\n",
      "         0.76663968, 0.5835251 ],\n",
      "        [0.74465519, 0.91936819, 0.73450499, 0.89278694, 0.87418683,\n",
      "         0.78604695, 0.60826605],\n",
      "        [0.89405581, 1.        , 0.87318409, 0.87825919, 0.87535036,\n",
      "         0.80023345, 0.6298655 ],\n",
      "        [0.90388889, 0.96073081, 0.72892227, 0.74097577, 0.83695762,\n",
      "         0.79111688, 0.63875432],\n",
      "        [0.6987249 , 0.82649256, 0.63928193, 0.72162674, 0.80400594,\n",
      "         0.78042609, 0.64538412],\n",
      "        [0.67836081, 0.70234101, 0.48823201, 0.49527382, 0.71579676,\n",
      "         0.73655651, 0.63337529],\n",
      "        [0.42904275, 0.53860315, 0.33756271, 0.46139695, 0.6431111 ,\n",
      "         0.69422427, 0.61961703],\n",
      "        [0.48309341, 0.65025707, 0.47484623, 0.55522432, 0.61800059,\n",
      "         0.67283966, 0.61446561],\n",
      "        [0.55858662, 0.72054818, 0.5205862 , 0.53175165, 0.59335804,\n",
      "         0.65113382, 0.60784849],\n",
      "        [0.5308    , 0.69878841, 0.47789131, 0.51227562, 0.57019163,\n",
      "         0.62977102, 0.60020266],\n",
      "        [0.47376777, 0.54875345, 0.33768952, 0.54494712, 0.56297891,\n",
      "         0.61672119, 0.59578222],\n",
      "        [0.53447951, 0.57717441, 0.43868562, 0.4674872 , 0.53569557,\n",
      "         0.59376211, 0.58551862],\n",
      "        [0.45993786, 0.48550405, 0.32246403, 0.4144516 , 0.50105443,\n",
      "         0.56617588, 0.57183326],\n",
      "        [0.39979705, 0.69796366, 0.35475482, 0.69060463, 0.55521163,\n",
      "         0.58531876, 0.58133497],\n",
      "        [0.69403043, 0.81145719, 0.63490458, 0.76920652, 0.61635303,\n",
      "         0.61360919, 0.59636469],\n",
      "        [0.77326656, 0.84875977, 0.67880494, 0.70703549, 0.6422623 ,\n",
      "         0.62798246, 0.60521835],\n",
      "        [0.70805055, 0.76070543, 0.47548057, 0.47909668, 0.59564355,\n",
      "         0.60507696, 0.59512862],\n",
      "        [0.42168373, 0.45740031, 0.35500863, 0.36103538, 0.52861265,\n",
      "         0.5675321 , 0.57640116],\n",
      "        [0.35735587, 0.45061222, 0.24950837, 0.25045993, 0.44914044,\n",
      "         0.51875177, 0.55032586],\n",
      "        [0.23060337, 0.2893485 , 0.17274629, 0.18905034, 0.37482898,\n",
      "         0.46802847, 0.52142382],\n",
      "        [0.18866972, 0.3066041 , 0.15371438, 0.19717061, 0.32406945,\n",
      "         0.42635803, 0.49548356],\n",
      "        [0.15650574, 0.17052596, 0.        , 0.13569752, 0.2702489 ,\n",
      "         0.38164103, 0.46670068],\n",
      "        [0.12319989, 0.17788499, 0.0119267 , 0.07149657, 0.21346252,\n",
      "         0.3339265 , 0.43508435],\n",
      "        [0.04485192, 0.21023918, 0.01351269, 0.20579842, 0.21127278,\n",
      "         0.31421449, 0.41674148]],\n",
      "\n",
      "       [[0.74465519, 0.91936819, 0.73450499, 0.89278694, 0.87418683,\n",
      "         0.78604695, 0.60826605],\n",
      "        [0.89405581, 1.        , 0.87318409, 0.87825919, 0.87535036,\n",
      "         0.80023345, 0.6298655 ],\n",
      "        [0.90388889, 0.96073081, 0.72892227, 0.74097577, 0.83695762,\n",
      "         0.79111688, 0.63875432],\n",
      "        [0.6987249 , 0.82649256, 0.63928193, 0.72162674, 0.80400594,\n",
      "         0.78042609, 0.64538412],\n",
      "        [0.67836081, 0.70234101, 0.48823201, 0.49527382, 0.71579676,\n",
      "         0.73655651, 0.63337529],\n",
      "        [0.42904275, 0.53860315, 0.33756271, 0.46139695, 0.6431111 ,\n",
      "         0.69422427, 0.61961703],\n",
      "        [0.48309341, 0.65025707, 0.47484623, 0.55522432, 0.61800059,\n",
      "         0.67283966, 0.61446561],\n",
      "        [0.55858662, 0.72054818, 0.5205862 , 0.53175165, 0.59335804,\n",
      "         0.65113382, 0.60784849],\n",
      "        [0.5308    , 0.69878841, 0.47789131, 0.51227562, 0.57019163,\n",
      "         0.62977102, 0.60020266],\n",
      "        [0.47376777, 0.54875345, 0.33768952, 0.54494712, 0.56297891,\n",
      "         0.61672119, 0.59578222],\n",
      "        [0.53447951, 0.57717441, 0.43868562, 0.4674872 , 0.53569557,\n",
      "         0.59376211, 0.58551862],\n",
      "        [0.45993786, 0.48550405, 0.32246403, 0.4144516 , 0.50105443,\n",
      "         0.56617588, 0.57183326],\n",
      "        [0.39979705, 0.69796366, 0.35475482, 0.69060463, 0.55521163,\n",
      "         0.58531876, 0.58133497],\n",
      "        [0.69403043, 0.81145719, 0.63490458, 0.76920652, 0.61635303,\n",
      "         0.61360919, 0.59636469],\n",
      "        [0.77326656, 0.84875977, 0.67880494, 0.70703549, 0.6422623 ,\n",
      "         0.62798246, 0.60521835],\n",
      "        [0.70805055, 0.76070543, 0.47548057, 0.47909668, 0.59564355,\n",
      "         0.60507696, 0.59512862],\n",
      "        [0.42168373, 0.45740031, 0.35500863, 0.36103538, 0.52861265,\n",
      "         0.5675321 , 0.57640116],\n",
      "        [0.35735587, 0.45061222, 0.24950837, 0.25045993, 0.44914044,\n",
      "         0.51875177, 0.55032586],\n",
      "        [0.23060337, 0.2893485 , 0.17274629, 0.18905034, 0.37482898,\n",
      "         0.46802847, 0.52142382],\n",
      "        [0.18866972, 0.3066041 , 0.15371438, 0.19717061, 0.32406945,\n",
      "         0.42635803, 0.49548356],\n",
      "        [0.15650574, 0.17052596, 0.        , 0.13569752, 0.2702489 ,\n",
      "         0.38164103, 0.46670068],\n",
      "        [0.12319989, 0.17788499, 0.0119267 , 0.07149657, 0.21346252,\n",
      "         0.3339265 , 0.43508435],\n",
      "        [0.04485192, 0.21023918, 0.01351269, 0.20579842, 0.21127278,\n",
      "         0.31421449, 0.41674148],\n",
      "        [0.21651974, 0.25705771, 0.16208846, 0.20002538, 0.20805924,\n",
      "         0.29664693, 0.39940419]]])>, <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
      "array([[0.25705771],\n",
      "       [0.23916767]])>)\n",
      "(<tf.Tensor: shape=(2, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.89563815, 1.        , 0.87507817, 0.88007747, 0.87721208,\n",
      "         0.80321709, 0.63539369],\n",
      "        [0.90532437, 0.96131732, 0.73297098, 0.74484446, 0.83939276,\n",
      "         0.79423669, 0.64414976],\n",
      "        [0.70322464, 0.82908401, 0.64466948, 0.72578442, 0.80693323,\n",
      "         0.78370557, 0.65068053],\n",
      "        [0.6831647 , 0.70678674, 0.49587559, 0.50281222, 0.72004152,\n",
      "         0.74049121, 0.63885106],\n",
      "        [0.43757036, 0.5454944 , 0.34745663, 0.46944132, 0.64844146,\n",
      "         0.69879122, 0.62529828],\n",
      "        [0.49081374, 0.6554807 , 0.48268973, 0.56186733, 0.62370599,\n",
      "         0.67772601, 0.62022381],\n",
      "        [0.56517941, 0.72472197, 0.52774655, 0.53874524, 0.59943149,\n",
      "         0.65634435, 0.61370552],\n",
      "        [0.5378078 , 0.70328719, 0.48568933, 0.51956009, 0.57661109,\n",
      "         0.63530062, 0.60617389],\n",
      "        [0.48162738, 0.5554931 , 0.34758155, 0.55174362, 0.5695061 ,\n",
      "         0.6224457 , 0.60181947],\n",
      "        [0.54143235, 0.58348957, 0.4470692 , 0.47544061, 0.54263025,\n",
      "         0.59982953, 0.59170916],\n",
      "        [0.46800403, 0.49318837, 0.33258346, 0.42319714, 0.5085065 ,\n",
      "         0.57265532, 0.5782282 ],\n",
      "        [0.40876146, 0.70247476, 0.36439196, 0.69522564, 0.56185483,\n",
      "         0.59151229, 0.58758799],\n",
      "        [0.69860028, 0.8142732 , 0.64035751, 0.77265356, 0.62208304,\n",
      "         0.61938018, 0.60239324],\n",
      "        [0.77665296, 0.85101864, 0.68360219, 0.7114111 , 0.64760534,\n",
      "         0.63353878, 0.61111467],\n",
      "        [0.712411  , 0.76427944, 0.4833146 , 0.4868767 , 0.60168287,\n",
      "         0.61097538, 0.60117563],\n",
      "        [0.43032124, 0.46550438, 0.36464198, 0.37057872, 0.53565311,\n",
      "         0.57399128, 0.58272788],\n",
      "        [0.36695416, 0.45881767, 0.26071744, 0.26165478, 0.45736788,\n",
      "         0.52593951, 0.55704203],\n",
      "        [0.24209479, 0.29996253, 0.18510184, 0.20116238, 0.38416631,\n",
      "         0.4759738 , 0.52857166],\n",
      "        [0.20078745, 0.31696041, 0.16635419, 0.20916138, 0.3341649 ,\n",
      "         0.43492574, 0.50301883],\n",
      "        [0.16910386, 0.18291467, 0.01493563, 0.14860643, 0.28114819,\n",
      "         0.39087661, 0.47466584],\n",
      "        [0.13629546, 0.19016379, 0.02668419, 0.08536436, 0.22520995,\n",
      "         0.34387473, 0.44352172],\n",
      "        [0.05911766, 0.22203475, 0.0282465 , 0.21766032, 0.22305292,\n",
      "         0.32445712, 0.42545281],\n",
      "        [0.22822151, 0.26815402, 0.1746032 , 0.21197351, 0.21988737,\n",
      "         0.30715195, 0.40837447],\n",
      "        [0.23815775, 0.25053118, 0.09630049, 0.10286219, 0.1864516 ,\n",
      "         0.27572276, 0.38393348]],\n",
      "\n",
      "       [[0.94175393, 1.        , 0.76246518, 0.77481644, 0.87316929,\n",
      "         0.82619617, 0.67006985],\n",
      "        [0.73152187, 0.86244572, 0.6706105 , 0.75498944, 0.83940362,\n",
      "         0.81524129, 0.67686342],\n",
      "        [0.71065473, 0.7352273 , 0.51582925, 0.52304501, 0.74901544,\n",
      "         0.77028801, 0.66455795],\n",
      "        [0.45517786, 0.56744469, 0.36143802, 0.48833129, 0.67453426,\n",
      "         0.72691006, 0.65045982],\n",
      "        [0.51056371, 0.68185675, 0.5021128 , 0.58447644, 0.64880345,\n",
      "         0.70499719, 0.64518115],\n",
      "        [0.5879218 , 0.75388423, 0.54898267, 0.56042394, 0.62355216,\n",
      "         0.68275515, 0.63840057],\n",
      "        [0.55944878, 0.73158694, 0.5052331 , 0.54046679, 0.59981349,\n",
      "         0.66086464, 0.63056587],\n",
      "        [0.5010077 , 0.57784572, 0.36156796, 0.57394537, 0.5924226 ,\n",
      "         0.64749244, 0.62603623],\n",
      "        [0.56321918, 0.60696875, 0.46505893, 0.49457199, 0.56446528,\n",
      "         0.62396622, 0.61551909],\n",
      "        [0.48683616, 0.5130339 , 0.34596636, 0.44022627, 0.52896842,\n",
      "         0.59569853, 0.60149566],\n",
      "        [0.42520971, 0.73074182, 0.37905482, 0.723201  , 0.58446344,\n",
      "         0.6153143 , 0.61123209],\n",
      "        [0.72671142, 0.84703893, 0.66612501, 0.80374456, 0.64711519,\n",
      "         0.64430357, 0.62663309],\n",
      "        [0.80790489, 0.88526298, 0.71110982, 0.74003774, 0.67366449,\n",
      "         0.6590319 , 0.63570546],\n",
      "        [0.74107788, 0.79503347, 0.50276281, 0.50646825, 0.62589414,\n",
      "         0.63556057, 0.62536648],\n",
      "        [0.44763704, 0.48423592, 0.3793149 , 0.38549053, 0.55720739,\n",
      "         0.59708826, 0.60617641],\n",
      "        [0.38172012, 0.47728014, 0.27120851, 0.27218357, 0.47577201,\n",
      "         0.54710292, 0.57945698],\n",
      "        [0.2518365 , 0.31203279, 0.1925502 , 0.20925701, 0.39962487,\n",
      "         0.49512663, 0.54984098],\n",
      "        [0.20886698, 0.32971466, 0.17304815, 0.21757787, 0.34761144,\n",
      "         0.45242682, 0.52325993],\n",
      "        [0.17590847, 0.19027502, 0.01553663, 0.15458624, 0.29246138,\n",
      "         0.40660519, 0.49376604],\n",
      "        [0.14177988, 0.19781584, 0.02775794, 0.08879936, 0.23427223,\n",
      "         0.35771198, 0.4613687 ],\n",
      "        [0.06149651, 0.23096926, 0.02938312, 0.22641881, 0.2320284 ,\n",
      "         0.33751303, 0.44257271],\n",
      "        [0.23740498, 0.27894434, 0.1816291 , 0.22050316, 0.22873547,\n",
      "         0.31951152, 0.42480715],\n",
      "        [0.24774104, 0.26061236, 0.10017555, 0.10700129, 0.19395428,\n",
      "         0.28681763, 0.39938268],\n",
      "        [0.0436846 , 0.10011048, 0.        , 0.0358838 , 0.14879128,\n",
      "         0.24821243, 0.37030277]]])>, <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
      "array([[0.09623794],\n",
      "       [0.11779235]])>)\n",
      "(<tf.Tensor: shape=(2, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.83750175, 0.97588306, 0.77312086, 0.86230604, 0.95152848,\n",
      "         0.92598985, 0.77972995],\n",
      "        [0.81544602, 0.84141825, 0.60952323, 0.61715   , 0.85599177,\n",
      "         0.87847603, 0.76672355],\n",
      "        [0.54541713, 0.6640787 , 0.44633784, 0.58045898, 0.77726811,\n",
      "         0.83262725, 0.75182239],\n",
      "        [0.60395777, 0.7850077 , 0.59502549, 0.68208059, 0.75007168,\n",
      "         0.80946623, 0.74624304],\n",
      "        [0.68572219, 0.86113788, 0.64456508, 0.65665804, 0.72338207,\n",
      "         0.78595728, 0.73907624],\n",
      "        [0.65562734, 0.83757053, 0.59832353, 0.63556414, 0.69829123,\n",
      "         0.76281987, 0.73079527],\n",
      "        [0.59385744, 0.67507218, 0.44647519, 0.67094966, 0.69047935,\n",
      "         0.74868599, 0.72600763],\n",
      "        [0.65961251, 0.70585406, 0.55586102, 0.58705515, 0.66092958,\n",
      "         0.72381971, 0.71489143],\n",
      "        [0.57887869, 0.60656866, 0.42998492, 0.52961389, 0.62341081,\n",
      "         0.69394189, 0.70006922],\n",
      "        [0.51374198, 0.83667727, 0.4649581 , 0.82870693, 0.68206684,\n",
      "         0.71467497, 0.71036024],\n",
      "        [0.83241731, 0.9595987 , 0.76837987, 0.91383827, 0.74828725,\n",
      "         0.74531548, 0.72663848],\n",
      "        [0.91823557, 1.        , 0.81592704, 0.8465027 , 0.77634881,\n",
      "         0.76088274, 0.73622762],\n",
      "        [0.84760208, 0.90463099, 0.59571253, 0.59962902, 0.72585744,\n",
      "         0.73607448, 0.72529973],\n",
      "        [0.53744678, 0.57613034, 0.46523299, 0.47176039, 0.65325828,\n",
      "         0.69541077, 0.70501658],\n",
      "        [0.46777522, 0.56877836, 0.35096883, 0.35199944, 0.56718433,\n",
      "         0.64257826, 0.67677521],\n",
      "        [0.33049339, 0.39411848, 0.26783013, 0.28548856, 0.48669982,\n",
      "         0.58764138, 0.64547228],\n",
      "        [0.28507632, 0.41280751, 0.24721724, 0.29428338, 0.4317237 ,\n",
      "         0.54250938, 0.61737717],\n",
      "        [0.25024048, 0.26542536, 0.08073382, 0.22770373, 0.37343228,\n",
      "         0.49407774, 0.58620329],\n",
      "        [0.21416792, 0.2733957 , 0.09365127, 0.15816961, 0.31192866,\n",
      "         0.44239957, 0.5519606 ],\n",
      "        [0.12931159, 0.30843755, 0.09536901, 0.3036279 , 0.30955701,\n",
      "         0.42105008, 0.53209398],\n",
      "        [0.31523984, 0.3591453 , 0.25628696, 0.2973753 , 0.30607652,\n",
      "         0.40202319, 0.51331649],\n",
      "        [0.32616465, 0.33976913, 0.1701938 , 0.17740833, 0.26931418,\n",
      "         0.36746706, 0.48644384],\n",
      "        [0.11048511, 0.17012502, 0.06431222, 0.10223997, 0.22157869,\n",
      "         0.32666289, 0.45570753],\n",
      "        [0.14312218, 0.18881405, 0.10038478, 0.18785212, 0.21194253,\n",
      "         0.30530739, 0.43427909]],\n",
      "\n",
      "       [[0.81962268, 0.84500713, 0.61836014, 0.62581432, 0.85925083,\n",
      "         0.88122625, 0.77200285],\n",
      "        [0.55570483, 0.67168096, 0.45886782, 0.58995365, 0.78230878,\n",
      "         0.83641508, 0.75743892],\n",
      "        [0.61292063, 0.78987321, 0.60419051, 0.68927545, 0.75572783,\n",
      "         0.81377821, 0.75198584],\n",
      "        [0.69283464, 0.86428048, 0.65260896, 0.66442825, 0.72964223,\n",
      "         0.7908013 , 0.74498123],\n",
      "        [0.66342086, 0.84124648, 0.6074139 , 0.64381172, 0.70511923,\n",
      "         0.76818751, 0.73688767],\n",
      "        [0.60304889, 0.68242565, 0.45900206, 0.67839643, 0.69748414,\n",
      "         0.7543735 , 0.73220837],\n",
      "        [0.66731585, 0.71251091, 0.56591237, 0.59640054, 0.66860311,\n",
      "         0.73006997, 0.72134375],\n",
      "        [0.58840912, 0.61547244, 0.44288498, 0.54025924, 0.63193343,\n",
      "         0.70086832, 0.70685698],\n",
      "        [0.52474653, 0.84037344, 0.47706668, 0.83258348, 0.68926202,\n",
      "         0.72113219, 0.7169151 ],\n",
      "        [0.83620988, 0.96051303, 0.77362169, 0.9157882 , 0.75398378,\n",
      "         0.75107927, 0.73282495],\n",
      "        [0.92008599, 1.        , 0.82009281, 0.84997651, 0.78141028,\n",
      "         0.76629423, 0.74219708],\n",
      "        [0.85105101, 0.9067893 , 0.60486199, 0.60868986, 0.73206159,\n",
      "         0.7420474 , 0.7315165 ],\n",
      "        [0.54791487, 0.58572297, 0.47733535, 0.48371503, 0.66110543,\n",
      "         0.70230396, 0.71169238],\n",
      "        [0.47982004, 0.57853738, 0.36565712, 0.3666644 , 0.57697942,\n",
      "         0.6506671 , 0.68409014],\n",
      "        [0.34564506, 0.40783024, 0.28439993, 0.30165873, 0.49831636,\n",
      "         0.59697351, 0.65349563],\n",
      "        [0.30125582, 0.42609631, 0.26425354, 0.31025452, 0.44458441,\n",
      "         0.55286289, 0.62603634],\n",
      "        [0.26720836, 0.28204958, 0.10153782, 0.24518164, 0.38761219,\n",
      "         0.50552732, 0.59556796],\n",
      "        [0.23195216, 0.28983955, 0.11416293, 0.17722115, 0.32750046,\n",
      "         0.45501867, 0.56210022],\n",
      "        [0.14901621, 0.32408836, 0.11584179, 0.31938756, 0.32518249,\n",
      "         0.43415235, 0.54268321],\n",
      "        [0.33073671, 0.37364854, 0.273118  , 0.31327646, 0.32178077,\n",
      "         0.41555606, 0.52433067],\n",
      "        [0.34141428, 0.35471087, 0.18897322, 0.19602448, 0.2858504 ,\n",
      "         0.38178197, 0.49806617],\n",
      "        [0.1306158 , 0.188906  , 0.08548786, 0.12255726, 0.23919522,\n",
      "         0.34190124, 0.46802546],\n",
      "        [0.16251426, 0.20717207, 0.12074405, 0.20623191, 0.22977713,\n",
      "         0.32102904, 0.44708197],\n",
      "        [0.21872268, 0.21993152, 0.02263109, 0.03142833, 0.17310604,\n",
      "         0.27647508, 0.41382968]]])>, <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
      "array([[0.20186894],\n",
      "       [0.13310049]])>)\n",
      "(<tf.Tensor: shape=(2, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.55570483, 0.67168096, 0.45886782, 0.58995365, 0.78230878,\n",
      "         0.83641508, 0.75743892],\n",
      "        [0.61292063, 0.78987321, 0.60419051, 0.68927545, 0.75572783,\n",
      "         0.81377821, 0.75198584],\n",
      "        [0.69283464, 0.86428048, 0.65260896, 0.66442825, 0.72964223,\n",
      "         0.7908013 , 0.74498123],\n",
      "        [0.66342086, 0.84124648, 0.6074139 , 0.64381172, 0.70511923,\n",
      "         0.76818751, 0.73688767],\n",
      "        [0.60304889, 0.68242565, 0.45900206, 0.67839643, 0.69748414,\n",
      "         0.7543735 , 0.73220837],\n",
      "        [0.66731585, 0.71251091, 0.56591237, 0.59640054, 0.66860311,\n",
      "         0.73006997, 0.72134375],\n",
      "        [0.58840912, 0.61547244, 0.44288498, 0.54025924, 0.63193343,\n",
      "         0.70086832, 0.70685698],\n",
      "        [0.52474653, 0.84037344, 0.47706668, 0.83258348, 0.68926202,\n",
      "         0.72113219, 0.7169151 ],\n",
      "        [0.83620988, 0.96051303, 0.77362169, 0.9157882 , 0.75398378,\n",
      "         0.75107927, 0.73282495],\n",
      "        [0.92008599, 1.        , 0.82009281, 0.84997651, 0.78141028,\n",
      "         0.76629423, 0.74219708],\n",
      "        [0.85105101, 0.9067893 , 0.60486199, 0.60868986, 0.73206159,\n",
      "         0.7420474 , 0.7315165 ],\n",
      "        [0.54791487, 0.58572297, 0.47733535, 0.48371503, 0.66110543,\n",
      "         0.70230396, 0.71169238],\n",
      "        [0.47982004, 0.57853738, 0.36565712, 0.3666644 , 0.57697942,\n",
      "         0.6506671 , 0.68409014],\n",
      "        [0.34564506, 0.40783024, 0.28439993, 0.30165873, 0.49831636,\n",
      "         0.59697351, 0.65349563],\n",
      "        [0.30125582, 0.42609631, 0.26425354, 0.31025452, 0.44458441,\n",
      "         0.55286289, 0.62603634],\n",
      "        [0.26720836, 0.28204958, 0.10153782, 0.24518164, 0.38761219,\n",
      "         0.50552732, 0.59556796],\n",
      "        [0.23195216, 0.28983955, 0.11416293, 0.17722115, 0.32750046,\n",
      "         0.45501867, 0.56210022],\n",
      "        [0.14901621, 0.32408836, 0.11584179, 0.31938756, 0.32518249,\n",
      "         0.43415235, 0.54268321],\n",
      "        [0.33073671, 0.37364854, 0.273118  , 0.31327646, 0.32178077,\n",
      "         0.41555606, 0.52433067],\n",
      "        [0.34141428, 0.35471087, 0.18897322, 0.19602448, 0.2858504 ,\n",
      "         0.38178197, 0.49806617],\n",
      "        [0.1306158 , 0.188906  , 0.08548786, 0.12255726, 0.23919522,\n",
      "         0.34190124, 0.46802546],\n",
      "        [0.16251426, 0.20717207, 0.12074405, 0.20623191, 0.22977713,\n",
      "         0.32102904, 0.44708197],\n",
      "        [0.21872268, 0.21993152, 0.02263109, 0.03142833, 0.17310604,\n",
      "         0.27647508, 0.41382968],\n",
      "        [0.00805854, 0.13310049, 0.        , 0.11973678, 0.15785768,\n",
      "         0.2523615 , 0.39030225]],\n",
      "\n",
      "       [[0.61292063, 0.78987321, 0.60419051, 0.68927545, 0.75572783,\n",
      "         0.81377821, 0.75198584],\n",
      "        [0.69283464, 0.86428048, 0.65260896, 0.66442825, 0.72964223,\n",
      "         0.7908013 , 0.74498123],\n",
      "        [0.66342086, 0.84124648, 0.6074139 , 0.64381172, 0.70511923,\n",
      "         0.76818751, 0.73688767],\n",
      "        [0.60304889, 0.68242565, 0.45900206, 0.67839643, 0.69748414,\n",
      "         0.7543735 , 0.73220837],\n",
      "        [0.66731585, 0.71251091, 0.56591237, 0.59640054, 0.66860311,\n",
      "         0.73006997, 0.72134375],\n",
      "        [0.58840912, 0.61547244, 0.44288498, 0.54025924, 0.63193343,\n",
      "         0.70086832, 0.70685698],\n",
      "        [0.52474653, 0.84037344, 0.47706668, 0.83258348, 0.68926202,\n",
      "         0.72113219, 0.7169151 ],\n",
      "        [0.83620988, 0.96051303, 0.77362169, 0.9157882 , 0.75398378,\n",
      "         0.75107927, 0.73282495],\n",
      "        [0.92008599, 1.        , 0.82009281, 0.84997651, 0.78141028,\n",
      "         0.76629423, 0.74219708],\n",
      "        [0.85105101, 0.9067893 , 0.60486199, 0.60868986, 0.73206159,\n",
      "         0.7420474 , 0.7315165 ],\n",
      "        [0.54791487, 0.58572297, 0.47733535, 0.48371503, 0.66110543,\n",
      "         0.70230396, 0.71169238],\n",
      "        [0.47982004, 0.57853738, 0.36565712, 0.3666644 , 0.57697942,\n",
      "         0.6506671 , 0.68409014],\n",
      "        [0.34564506, 0.40783024, 0.28439993, 0.30165873, 0.49831636,\n",
      "         0.59697351, 0.65349563],\n",
      "        [0.30125582, 0.42609631, 0.26425354, 0.31025452, 0.44458441,\n",
      "         0.55286289, 0.62603634],\n",
      "        [0.26720836, 0.28204958, 0.10153782, 0.24518164, 0.38761219,\n",
      "         0.50552732, 0.59556796],\n",
      "        [0.23195216, 0.28983955, 0.11416293, 0.17722115, 0.32750046,\n",
      "         0.45501867, 0.56210022],\n",
      "        [0.14901621, 0.32408836, 0.11584179, 0.31938756, 0.32518249,\n",
      "         0.43415235, 0.54268321],\n",
      "        [0.33073671, 0.37364854, 0.273118  , 0.31327646, 0.32178077,\n",
      "         0.41555606, 0.52433067],\n",
      "        [0.34141428, 0.35471087, 0.18897322, 0.19602448, 0.2858504 ,\n",
      "         0.38178197, 0.49806617],\n",
      "        [0.1306158 , 0.188906  , 0.08548786, 0.12255726, 0.23919522,\n",
      "         0.34190124, 0.46802546],\n",
      "        [0.16251426, 0.20717207, 0.12074405, 0.20623191, 0.22977713,\n",
      "         0.32102904, 0.44708197],\n",
      "        [0.21872268, 0.21993152, 0.02263109, 0.03142833, 0.17310604,\n",
      "         0.27647508, 0.41382968],\n",
      "        [0.00805854, 0.13310049, 0.        , 0.11973678, 0.15785768,\n",
      "         0.2523615 , 0.39030225],\n",
      "        [0.09878445, 0.11597613, 0.02753335, 0.11463296, 0.14550776,\n",
      "         0.23117249, 0.36824871]]])>, <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
      "array([[0.11597613],\n",
      "       [0.27459541]])>)\n",
      "(<tf.Tensor: shape=(2, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.69283464, 0.86428048, 0.65260896, 0.66442825, 0.72964223,\n",
      "         0.7908013 , 0.74498123],\n",
      "        [0.66342086, 0.84124648, 0.6074139 , 0.64381172, 0.70511923,\n",
      "         0.76818751, 0.73688767],\n",
      "        [0.60304889, 0.68242565, 0.45900206, 0.67839643, 0.69748414,\n",
      "         0.7543735 , 0.73220837],\n",
      "        [0.66731585, 0.71251091, 0.56591237, 0.59640054, 0.66860311,\n",
      "         0.73006997, 0.72134375],\n",
      "        [0.58840912, 0.61547244, 0.44288498, 0.54025924, 0.63193343,\n",
      "         0.70086832, 0.70685698],\n",
      "        [0.52474653, 0.84037344, 0.47706668, 0.83258348, 0.68926202,\n",
      "         0.72113219, 0.7169151 ],\n",
      "        [0.83620988, 0.96051303, 0.77362169, 0.9157882 , 0.75398378,\n",
      "         0.75107927, 0.73282495],\n",
      "        [0.92008599, 1.        , 0.82009281, 0.84997651, 0.78141028,\n",
      "         0.76629423, 0.74219708],\n",
      "        [0.85105101, 0.9067893 , 0.60486199, 0.60868986, 0.73206159,\n",
      "         0.7420474 , 0.7315165 ],\n",
      "        [0.54791487, 0.58572297, 0.47733535, 0.48371503, 0.66110543,\n",
      "         0.70230396, 0.71169238],\n",
      "        [0.47982004, 0.57853738, 0.36565712, 0.3666644 , 0.57697942,\n",
      "         0.6506671 , 0.68409014],\n",
      "        [0.34564506, 0.40783024, 0.28439993, 0.30165873, 0.49831636,\n",
      "         0.59697351, 0.65349563],\n",
      "        [0.30125582, 0.42609631, 0.26425354, 0.31025452, 0.44458441,\n",
      "         0.55286289, 0.62603634],\n",
      "        [0.26720836, 0.28204958, 0.10153782, 0.24518164, 0.38761219,\n",
      "         0.50552732, 0.59556796],\n",
      "        [0.23195216, 0.28983955, 0.11416293, 0.17722115, 0.32750046,\n",
      "         0.45501867, 0.56210022],\n",
      "        [0.14901621, 0.32408836, 0.11584179, 0.31938756, 0.32518249,\n",
      "         0.43415235, 0.54268321],\n",
      "        [0.33073671, 0.37364854, 0.273118  , 0.31327646, 0.32178077,\n",
      "         0.41555606, 0.52433067],\n",
      "        [0.34141428, 0.35471087, 0.18897322, 0.19602448, 0.2858504 ,\n",
      "         0.38178197, 0.49806617],\n",
      "        [0.1306158 , 0.188906  , 0.08548786, 0.12255726, 0.23919522,\n",
      "         0.34190124, 0.46802546],\n",
      "        [0.16251426, 0.20717207, 0.12074405, 0.20623191, 0.22977713,\n",
      "         0.32102904, 0.44708197],\n",
      "        [0.21872268, 0.21993152, 0.02263109, 0.03142833, 0.17310604,\n",
      "         0.27647508, 0.41382968],\n",
      "        [0.00805854, 0.13310049, 0.        , 0.11973678, 0.15785768,\n",
      "         0.2523615 , 0.39030225],\n",
      "        [0.09878445, 0.11597613, 0.02753335, 0.11463296, 0.14550776,\n",
      "         0.23117249, 0.36824871],\n",
      "        [0.15089653, 0.27459541, 0.13075014, 0.21919281, 0.16656063,\n",
      "         0.22932946, 0.35632424]],\n",
      "\n",
      "       [[0.66342086, 0.84124648, 0.6074139 , 0.64381172, 0.70511923,\n",
      "         0.76818751, 0.73688767],\n",
      "        [0.60304889, 0.68242565, 0.45900206, 0.67839643, 0.69748414,\n",
      "         0.7543735 , 0.73220837],\n",
      "        [0.66731585, 0.71251091, 0.56591237, 0.59640054, 0.66860311,\n",
      "         0.73006997, 0.72134375],\n",
      "        [0.58840912, 0.61547244, 0.44288498, 0.54025924, 0.63193343,\n",
      "         0.70086832, 0.70685698],\n",
      "        [0.52474653, 0.84037344, 0.47706668, 0.83258348, 0.68926202,\n",
      "         0.72113219, 0.7169151 ],\n",
      "        [0.83620988, 0.96051303, 0.77362169, 0.9157882 , 0.75398378,\n",
      "         0.75107927, 0.73282495],\n",
      "        [0.92008599, 1.        , 0.82009281, 0.84997651, 0.78141028,\n",
      "         0.76629423, 0.74219708],\n",
      "        [0.85105101, 0.9067893 , 0.60486199, 0.60868986, 0.73206159,\n",
      "         0.7420474 , 0.7315165 ],\n",
      "        [0.54791487, 0.58572297, 0.47733535, 0.48371503, 0.66110543,\n",
      "         0.70230396, 0.71169238],\n",
      "        [0.47982004, 0.57853738, 0.36565712, 0.3666644 , 0.57697942,\n",
      "         0.6506671 , 0.68409014],\n",
      "        [0.34564506, 0.40783024, 0.28439993, 0.30165873, 0.49831636,\n",
      "         0.59697351, 0.65349563],\n",
      "        [0.30125582, 0.42609631, 0.26425354, 0.31025452, 0.44458441,\n",
      "         0.55286289, 0.62603634],\n",
      "        [0.26720836, 0.28204958, 0.10153782, 0.24518164, 0.38761219,\n",
      "         0.50552732, 0.59556796],\n",
      "        [0.23195216, 0.28983955, 0.11416293, 0.17722115, 0.32750046,\n",
      "         0.45501867, 0.56210022],\n",
      "        [0.14901621, 0.32408836, 0.11584179, 0.31938756, 0.32518249,\n",
      "         0.43415235, 0.54268321],\n",
      "        [0.33073671, 0.37364854, 0.273118  , 0.31327646, 0.32178077,\n",
      "         0.41555606, 0.52433067],\n",
      "        [0.34141428, 0.35471087, 0.18897322, 0.19602448, 0.2858504 ,\n",
      "         0.38178197, 0.49806617],\n",
      "        [0.1306158 , 0.188906  , 0.08548786, 0.12255726, 0.23919522,\n",
      "         0.34190124, 0.46802546],\n",
      "        [0.16251426, 0.20717207, 0.12074405, 0.20623191, 0.22977713,\n",
      "         0.32102904, 0.44708197],\n",
      "        [0.21872268, 0.21993152, 0.02263109, 0.03142833, 0.17310604,\n",
      "         0.27647508, 0.41382968],\n",
      "        [0.00805854, 0.13310049, 0.        , 0.11973678, 0.15785768,\n",
      "         0.2523615 , 0.39030225],\n",
      "        [0.09878445, 0.11597613, 0.02753335, 0.11463296, 0.14550776,\n",
      "         0.23117249, 0.36824871],\n",
      "        [0.15089653, 0.27459541, 0.13075014, 0.21919281, 0.16656063,\n",
      "         0.22932946, 0.35632424],\n",
      "        [0.19904642, 0.28131088, 0.16271572, 0.27587137, 0.19779227,\n",
      "         0.23648976, 0.34988801]]])>, <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
      "array([[0.28131088],\n",
      "       [0.3504802 ]])>)\n",
      "(<tf.Tensor: shape=(1, 24, 7), dtype=float64, numpy=\n",
      "array([[[0.60304889, 0.68242565, 0.45900206, 0.67839643, 0.69748414,\n",
      "         0.7543735 , 0.73220837],\n",
      "        [0.66731585, 0.71251091, 0.56591237, 0.59640054, 0.66860311,\n",
      "         0.73006997, 0.72134375],\n",
      "        [0.58840912, 0.61547244, 0.44288498, 0.54025924, 0.63193343,\n",
      "         0.70086832, 0.70685698],\n",
      "        [0.52474653, 0.84037344, 0.47706668, 0.83258348, 0.68926202,\n",
      "         0.72113219, 0.7169151 ],\n",
      "        [0.83620988, 0.96051303, 0.77362169, 0.9157882 , 0.75398378,\n",
      "         0.75107927, 0.73282495],\n",
      "        [0.92008599, 1.        , 0.82009281, 0.84997651, 0.78141028,\n",
      "         0.76629423, 0.74219708],\n",
      "        [0.85105101, 0.9067893 , 0.60486199, 0.60868986, 0.73206159,\n",
      "         0.7420474 , 0.7315165 ],\n",
      "        [0.54791487, 0.58572297, 0.47733535, 0.48371503, 0.66110543,\n",
      "         0.70230396, 0.71169238],\n",
      "        [0.47982004, 0.57853738, 0.36565712, 0.3666644 , 0.57697942,\n",
      "         0.6506671 , 0.68409014],\n",
      "        [0.34564506, 0.40783024, 0.28439993, 0.30165873, 0.49831636,\n",
      "         0.59697351, 0.65349563],\n",
      "        [0.30125582, 0.42609631, 0.26425354, 0.31025452, 0.44458441,\n",
      "         0.55286289, 0.62603634],\n",
      "        [0.26720836, 0.28204958, 0.10153782, 0.24518164, 0.38761219,\n",
      "         0.50552732, 0.59556796],\n",
      "        [0.23195216, 0.28983955, 0.11416293, 0.17722115, 0.32750046,\n",
      "         0.45501867, 0.56210022],\n",
      "        [0.14901621, 0.32408836, 0.11584179, 0.31938756, 0.32518249,\n",
      "         0.43415235, 0.54268321],\n",
      "        [0.33073671, 0.37364854, 0.273118  , 0.31327646, 0.32178077,\n",
      "         0.41555606, 0.52433067],\n",
      "        [0.34141428, 0.35471087, 0.18897322, 0.19602448, 0.2858504 ,\n",
      "         0.38178197, 0.49806617],\n",
      "        [0.1306158 , 0.188906  , 0.08548786, 0.12255726, 0.23919522,\n",
      "         0.34190124, 0.46802546],\n",
      "        [0.16251426, 0.20717207, 0.12074405, 0.20623191, 0.22977713,\n",
      "         0.32102904, 0.44708197],\n",
      "        [0.21872268, 0.21993152, 0.02263109, 0.03142833, 0.17310604,\n",
      "         0.27647508, 0.41382968],\n",
      "        [0.00805854, 0.13310049, 0.        , 0.11973678, 0.15785768,\n",
      "         0.2523615 , 0.39030225],\n",
      "        [0.09878445, 0.11597613, 0.02753335, 0.11463296, 0.14550776,\n",
      "         0.23117249, 0.36824871],\n",
      "        [0.15089653, 0.27459541, 0.13075014, 0.21919281, 0.16656063,\n",
      "         0.22932946, 0.35632424],\n",
      "        [0.19904642, 0.28131088, 0.16271572, 0.27587137, 0.19779227,\n",
      "         0.23648976, 0.34988801],\n",
      "        [0.27714732, 0.3504802 , 0.26425354, 0.33134108, 0.23594907,\n",
      "         0.25108227, 0.34840425]]])>, <tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[0.31314212]])>)\n"
     ]
    }
   ],
   "source": [
    "from training import GetTensoredDataset\n",
    "\n",
    "GetTensoredValidDataset = GetTensoredDataset()\n",
    "\n",
    "GetTensoredValidDataset.fit(\n",
    "    window_size=25, batch_size=2, train=False, debug=False)\n",
    "\n",
    "x_valid_tensors, labels = GetTensoredValidDataset.transform(x_valid)\n",
    "\n",
    "for batch in x_valid_tensors:\n",
    "    if c < 3:\n",
    "        print(batch)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model Training</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "model_name = f'{saved_models}/{str.upper(ticker)}_{formation_window}_{target_window}_{window_size}_{split_ratio}_{start_date}_{end_date}.h5'\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=6, mode='min', restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=2, min_lr=10e-15,\n",
    "                              verbose=1)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(monitor='val_loss',\n",
    "                                   filepath=model_name,\n",
    "                                   save_best_only=True)\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr, model_checkpoint]\n",
    "\n",
    "\n",
    "def sign_penalty(y_true, y_pred):\n",
    "    penalty = 100.\n",
    "    loss = tf.where(tf.less(y_true*y_pred, 0),\n",
    "                    penalty * tf.square(y_true-y_pred),\n",
    "                    tf.square(y_true - y_pred)\n",
    "                    )\n",
    "\n",
    "    return(tf.reduce_mean(loss, axis=-1))\n",
    "\n",
    "\n",
    "tf.keras.losses.sign_penalty = sign_penalty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ModelGeneral():\n",
    "# #tf.random.set_seed(7788)\n",
    "\n",
    "#     # model = tf.keras.models.Sequential([\n",
    "\n",
    "#     #     tf.keras.layers.Conv1D(filters=256, kernel_size=10,\n",
    "#     #                     strides=1, padding=\"same\",\n",
    "#     #                     activation=tf.nn.selu,\n",
    "#     #                     input_shape=[None, 7]),\n",
    "#     # #     tf.keras.layers.Conv1D(filters=512, kernel_size=10,\n",
    "#     # #                       strides=1, padding=\"same\",\n",
    "#     # #                       activation=tf.nn.selu,\n",
    "#     # #                       input_shape=[None, 7]),\n",
    "\n",
    "#     #     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #     #tf.keras.layers.Dropout(0.2),\n",
    "#     #     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #     #tf.keras.layers.Dropout(0.2),\n",
    "#     #     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #     #tf.keras.layers.Dropout(0.2),\n",
    "#     #     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #     # tf.keras.layers.Dropout(0.2),\n",
    "#     #     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #     #tf.keras.layers.Dropout(0.2),\n",
    "#     #     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #     #tf.keras.layers.Dropout(0.2),\n",
    "#     #     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #     #tf.keras.layers.Dropout(0.2),\n",
    "#     #     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #     #tf.keras.layers.Dropout(0.2),\n",
    "#     #     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
    "#     #     #tf.keras.layers.Dropout(0.2),\n",
    "#     #     tf.keras.layers.Dense(1024, activation=tf.nn.selu),\n",
    "#     #     #tf.keras.layers.Dropout(0.2),\n",
    "#     #     tf.keras.layers.Dense(1024, activation=tf.nn.selu),\n",
    "#     #     #tf.keras.layers.Dropout(0.2),\n",
    "#     #     tf.keras.layers.Dense(1024, activation=tf.nn.selu),\n",
    "#     #     #tf.keras.layers.Dropout(0.2),\n",
    "#     #     tf.keras.layers.Dense(1024, activation=tf.nn.selu),\n",
    "#     #     #tf.keras.layers.Dropout(0.2),\n",
    "#     #     tf.keras.layers.Dense(16, activation=tf.nn.selu),\n",
    "#     #     tf.keras.layers.Dense(16, activation=tf.nn.selu),\n",
    "#     #     tf.keras.layers.Dense(4, activation=tf.nn.selu),\n",
    "#     #     tf.keras.layers.Dense(3, activation=tf.nn.selu), #4\n",
    "#     #     tf.keras.layers.Dense(1,activation=tf.nn.relu),\n",
    "#     # ])\n",
    "\n",
    "#     # #optimizer1 = tf.keras.optimizers.SGD(learning_rate=10e-7, momentum=0.9)\n",
    "#     # #optimizer2 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "#     # #optimizer3 = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False)\n",
    "#     # #optimizer4 = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07, name='Adadelta')\n",
    "\n",
    "#     # optimizer5 = tf.keras.optimizers.Adagrad(learning_rate=0.0001, initial_accumulator_value=0.1, epsilon=1e-07,name='Adagrad')\n",
    "\n",
    "\n",
    "#     # model.compile(loss=sign_penalty,\n",
    "#     #             optimizer=optimizer5,\n",
    "#     #             )\n",
    "\n",
    "#     # model.fit(train_set, epochs=120,callbacks=[callbacks],validation_data=val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1200\n",
      "12/12 [==============================] - 13s 417ms/step - loss: 0.0255 - val_loss: 0.0836 - lr: 0.0050\n",
      "Epoch 2/1200\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 0.0269 - val_loss: 0.0749 - lr: 0.0050\n",
      "Epoch 3/1200\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 0.0264 - val_loss: 0.0724 - lr: 0.0050\n",
      "Epoch 4/1200\n",
      "12/12 [==============================] - 1s 43ms/step - loss: 0.0254 - val_loss: 0.0680 - lr: 0.0050\n",
      "Epoch 5/1200\n",
      "12/12 [==============================] - 0s 36ms/step - loss: 0.0247 - val_loss: 0.0719 - lr: 0.0050\n",
      "Epoch 6/1200\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0205\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "12/12 [==============================] - 0s 38ms/step - loss: 0.0233 - val_loss: 0.0730 - lr: 0.0050\n",
      "Epoch 7/1200\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0245 - val_loss: 0.0792 - lr: 1.0000e-03\n",
      "Epoch 8/1200\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0206\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0001999999862164259.\n",
      "12/12 [==============================] - 0s 38ms/step - loss: 0.0227 - val_loss: 0.0836 - lr: 1.0000e-03\n",
      "Epoch 9/1200\n",
      "12/12 [==============================] - 0s 38ms/step - loss: 0.0221 - val_loss: 0.0845 - lr: 2.0000e-04\n",
      "Epoch 10/1200\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0198\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 3.9999996079131965e-05.\n",
      "12/12 [==============================] - 0s 37ms/step - loss: 0.0219 - val_loss: 0.0854 - lr: 2.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2744289d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(7788)\n",
    "np.random.seed(7788)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "    tf.keras.layers.Conv1D(filters=8, kernel_size=1,\n",
    "                           strides=1, padding=\"same\",\n",
    "                           activation=tf.nn.selu,\n",
    "                           input_shape=[None, 7]),\n",
    "    tf.keras.layers.Conv1D(filters=16, kernel_size=1,\n",
    "                           strides=1, padding=\"same\",\n",
    "                           activation=tf.nn.selu,\n",
    "                           #input_shape=[None, 7]\n",
    "                           ),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=10,\n",
    "                           strides=1, padding=\"same\",\n",
    "                           activation=tf.nn.selu,\n",
    "                           #input_shape=[None, 7]\n",
    "                           ),\n",
    "\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(9, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(9)),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.selu),\n",
    "    tf.keras.layers.Dense(3, activation=tf.nn.selu),\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.selu),        \n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.relu),\n",
    "])\n",
    "\n",
    "def mrk_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "\n",
    "    tf.keras.layers.Conv1D(filters=8, kernel_size=1,\n",
    "                           strides=1, padding=\"same\",\n",
    "                           activation=tf.nn.selu,\n",
    "                           input_shape=[None, 7]),\n",
    "    tf.keras.layers.Conv1D(filters=16, kernel_size=1,\n",
    "                           strides=1, padding=\"same\",\n",
    "                           activation=tf.nn.selu,\n",
    "                           #input_shape=[None, 7]\n",
    "                           ),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=10,\n",
    "                           strides=1, padding=\"same\",\n",
    "                           activation=tf.nn.selu,\n",
    "                           #input_shape=[None, 7]\n",
    "                           ),\n",
    "\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(9, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(9)),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.selu),\n",
    "    tf.keras.layers.Dense(3, activation=tf.nn.selu),\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.selu),        \n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.relu),\n",
    "    ])\n",
    "    optimizer2 = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0009, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "    optimizer5 = tf.keras.optimizers.Adagrad(\n",
    "        learning_rate=0.005, initial_accumulator_value=5, epsilon=1e-07, name='Adagrad')\n",
    "\n",
    "    model.compile(loss=sign_penalty,\n",
    "                optimizer=optimizer5,\n",
    "                )\n",
    "\n",
    "    model.fit(x_train_tensors, epochs=1200, callbacks=[\n",
    "            callbacks], validation_data=x_valid_tensors)\n",
    "    return model\n",
    "\n",
    "optimizer2 = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0009, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "optimizer5 = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.005, initial_accumulator_value=5, epsilon=1e-07, name='Adagrad')\n",
    "\n",
    "model.compile(loss=sign_penalty,\n",
    "              optimizer=optimizer5,\n",
    "              )\n",
    "\n",
    "model.fit(x_train_tensors, epochs=1200, callbacks=[\n",
    "          callbacks], validation_data=x_valid_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Model folders\n",
    "# trained_models\n",
    "# saved_models\n",
    "\n",
    "# #Model name\n",
    "model_name = f'{saved_models}/{str.upper(ticker)}_{formation_window}_{target_window}_{window_size}_{split_ratio}_{start_date}_{end_date}.h5'\n",
    "\n",
    "model = tf.keras.models.load_model(model_name, custom_objects={\n",
    "                                            'sign_penalty': sign_penalty})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 2s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.6238662, 0.5978768], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_forecast(model, series, window_size, debug):\n",
    "    \"\"\"\n",
    "    Get model, data and window size as an input. \n",
    "    Make prediction window is subtracted by 1, since we do not need label in window, \n",
    "    label value is skipped\n",
    "    \"\"\"\n",
    "    c = 0\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size-1, shift=window_size, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "\n",
    "    if debug == True:\n",
    "        # This block of code will print out data on which is made prediction\n",
    "        for item in ds:\n",
    "            c += 1\n",
    "            if c < 3:\n",
    "                print(\"\\n\"+str(c) + \" prediction:\\n \", item)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    ds = ds.batch(1).prefetch(1)\n",
    "    forecast = model.predict(ds)\n",
    "    forecast2 = np.squeeze(forecast)\n",
    "    return forecast2\n",
    "\n",
    "\n",
    "forecast = model_forecast(model, x_valid, window_size=window_size, debug=False)\n",
    "forecast[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr = x_valid.iloc[:24, :].to_numpy()\n",
    "# pr = np.array([pr])\n",
    "# pr = np.array([pr])\n",
    "# pred = tf.data.Dataset.from_tensor_slices(pr)\n",
    "# predict = model.predict(pred)\n",
    "# print(\"Raw prediction: \", predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------> ReverseNormalization completed\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(575, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ReverseNormalization\n",
    "\n",
    "ReverseNormalization = ReverseNormalization()\n",
    "\n",
    "ReverseNormalization.fit(forecasts=forecast, labels=labels,\n",
    "                         x_valid=x_valid, x_valid_x=x_valid_x, window_size=25, debug=False)\n",
    "\n",
    "df = ReverseNormalization.transform()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------> GetFinalDataframe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from final_evaluation import GetFinalDataframe\n",
    "\n",
    "GetFinalDataframe = GetFinalDataframe()\n",
    "\n",
    "GetFinalDataframe.fit(dates=Dates,\n",
    "                      x_valid=x_valid)\n",
    "\n",
    "reversed_df = GetFinalDataframe.transform(df)\n",
    "#df1.head(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "period: 2021-09-20 - 2022-08-01\n",
      "Formations:  23\n",
      "Entry Candle:  Current Open\n",
      "\n",
      "Total Trades:  23\n",
      "Profit Trades:  12\n",
      "Loss Trades:  11\n",
      "\n",
      "Win Ratio: 52.0 %\n",
      "Loss Ratio: 48 %\n",
      "\n",
      "Average profit per trade:  382\n",
      "\n",
      "Gross profit:  8787\n",
      "Gross loss:  -6814\n",
      "\n",
      "Net profit:  1973\n"
     ]
    }
   ],
   "source": [
    "from final_evaluation import GetModelPerformance\n",
    "\n",
    "GetModelPerformance = GetModelPerformance()\n",
    "\n",
    "GetModelPerformance.fit(acceptance=0,\n",
    "                        penalization=0,\n",
    "                        entry_candle='Current Open',\n",
    "                        budget=10000,\n",
    "                        window_size=25,\n",
    "                        export_excel=True,\n",
    "                        excel_path = excel_reports)\n",
    "\n",
    "trades_df = GetModelPerformance.transform(reversed_df)\n",
    "#trades_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade = 1\n",
    "budget = 10000\n",
    "entry_candle = 'Current Open'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trade:  15\n",
      "Period: 2022-01-03 - 2022-06-13\n",
      "\n",
      "Budget:  10000\n",
      "\n",
      "Entry price:  86.05\n",
      "Label (target):  95.72\n",
      "Model prediction:  91.55\n",
      "Market Change: 5.5 $\n",
      "Profit: 639.16 $\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x414 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>EMA6</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>EMA24</th>\n",
       "      <th>labels</th>\n",
       "      <th>prediction</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>profit</th>\n",
       "      <th>trade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>76.580002</td>\n",
       "      <td>80.529999</td>\n",
       "      <td>75.349998</td>\n",
       "      <td>80.300003</td>\n",
       "      <td>77.562326</td>\n",
       "      <td>77.783698</td>\n",
       "      <td>77.671197</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>80.540001</td>\n",
       "      <td>82.559998</td>\n",
       "      <td>79.669998</td>\n",
       "      <td>81.379997</td>\n",
       "      <td>78.653089</td>\n",
       "      <td>78.336974</td>\n",
       "      <td>77.967901</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-01-10</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>81.099998</td>\n",
       "      <td>82.160004</td>\n",
       "      <td>79.239998</td>\n",
       "      <td>79.980003</td>\n",
       "      <td>79.032208</td>\n",
       "      <td>78.589748</td>\n",
       "      <td>78.128869</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-01-17</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>79.769997</td>\n",
       "      <td>81.599998</td>\n",
       "      <td>76.650002</td>\n",
       "      <td>80.900002</td>\n",
       "      <td>79.565863</td>\n",
       "      <td>78.945172</td>\n",
       "      <td>78.350560</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-01-24</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>81.019997</td>\n",
       "      <td>82.300003</td>\n",
       "      <td>78.199997</td>\n",
       "      <td>78.559998</td>\n",
       "      <td>79.278473</td>\n",
       "      <td>78.885914</td>\n",
       "      <td>78.367315</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>78.790001</td>\n",
       "      <td>78.919998</td>\n",
       "      <td>75.820000</td>\n",
       "      <td>76.639999</td>\n",
       "      <td>78.524623</td>\n",
       "      <td>78.540389</td>\n",
       "      <td>78.229130</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-02-07</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>76.629997</td>\n",
       "      <td>78.190002</td>\n",
       "      <td>75.110001</td>\n",
       "      <td>76.370003</td>\n",
       "      <td>77.909017</td>\n",
       "      <td>78.206483</td>\n",
       "      <td>78.080399</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>76.459999</td>\n",
       "      <td>77.290001</td>\n",
       "      <td>72.879997</td>\n",
       "      <td>76.320000</td>\n",
       "      <td>77.455012</td>\n",
       "      <td>77.916255</td>\n",
       "      <td>77.939567</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-02-21</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>75.830002</td>\n",
       "      <td>77.860001</td>\n",
       "      <td>74.779999</td>\n",
       "      <td>77.830002</td>\n",
       "      <td>77.562152</td>\n",
       "      <td>77.902985</td>\n",
       "      <td>77.930802</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>77.010002</td>\n",
       "      <td>79.629997</td>\n",
       "      <td>76.080002</td>\n",
       "      <td>78.260002</td>\n",
       "      <td>77.761538</td>\n",
       "      <td>77.957911</td>\n",
       "      <td>77.957138</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>77.900002</td>\n",
       "      <td>79.739998</td>\n",
       "      <td>77.309998</td>\n",
       "      <td>79.110001</td>\n",
       "      <td>78.146813</td>\n",
       "      <td>78.135156</td>\n",
       "      <td>78.049367</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-03-14</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>79.669998</td>\n",
       "      <td>81.379997</td>\n",
       "      <td>78.769997</td>\n",
       "      <td>81.339996</td>\n",
       "      <td>79.059151</td>\n",
       "      <td>78.628208</td>\n",
       "      <td>78.312618</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-03-21</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>81.370003</td>\n",
       "      <td>83.580002</td>\n",
       "      <td>80.730003</td>\n",
       "      <td>83.519997</td>\n",
       "      <td>80.333678</td>\n",
       "      <td>79.380791</td>\n",
       "      <td>78.729208</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-03-28</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>83.500000</td>\n",
       "      <td>87.839996</td>\n",
       "      <td>82.730003</td>\n",
       "      <td>87.680000</td>\n",
       "      <td>82.432627</td>\n",
       "      <td>80.657592</td>\n",
       "      <td>79.445271</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>88.320000</td>\n",
       "      <td>89.480003</td>\n",
       "      <td>84.830002</td>\n",
       "      <td>86.910004</td>\n",
       "      <td>83.711878</td>\n",
       "      <td>81.619502</td>\n",
       "      <td>80.042450</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>86.419998</td>\n",
       "      <td>87.449997</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>84.589996</td>\n",
       "      <td>83.962769</td>\n",
       "      <td>82.076501</td>\n",
       "      <td>80.406254</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-04-18</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>84.589996</td>\n",
       "      <td>90.010002</td>\n",
       "      <td>83.529999</td>\n",
       "      <td>88.690002</td>\n",
       "      <td>85.313407</td>\n",
       "      <td>83.093963</td>\n",
       "      <td>81.068953</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>88.720001</td>\n",
       "      <td>88.919998</td>\n",
       "      <td>86.360001</td>\n",
       "      <td>88.389999</td>\n",
       "      <td>86.192433</td>\n",
       "      <td>83.908738</td>\n",
       "      <td>81.654637</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-05-02</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>87.500000</td>\n",
       "      <td>91.050003</td>\n",
       "      <td>86.910004</td>\n",
       "      <td>90.410004</td>\n",
       "      <td>87.397453</td>\n",
       "      <td>84.908932</td>\n",
       "      <td>82.355066</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-05-09</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>90.790001</td>\n",
       "      <td>94.559998</td>\n",
       "      <td>90.680000</td>\n",
       "      <td>93.550003</td>\n",
       "      <td>89.155325</td>\n",
       "      <td>86.238328</td>\n",
       "      <td>83.250661</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-05-16</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>93.589996</td>\n",
       "      <td>94.919998</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>93.080002</td>\n",
       "      <td>90.276661</td>\n",
       "      <td>87.290893</td>\n",
       "      <td>84.037009</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-05-23</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>91.910004</td>\n",
       "      <td>92.620003</td>\n",
       "      <td>88.739998</td>\n",
       "      <td>89.910004</td>\n",
       "      <td>90.171902</td>\n",
       "      <td>87.693833</td>\n",
       "      <td>84.506848</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-05-30</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>90.169998</td>\n",
       "      <td>90.820000</td>\n",
       "      <td>86.709999</td>\n",
       "      <td>87.180000</td>\n",
       "      <td>89.317073</td>\n",
       "      <td>87.614782</td>\n",
       "      <td>84.720700</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-06-06</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>85.709999</td>\n",
       "      <td>86.510002</td>\n",
       "      <td>83.050003</td>\n",
       "      <td>84.620003</td>\n",
       "      <td>87.975053</td>\n",
       "      <td>87.154047</td>\n",
       "      <td>84.712645</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-06-13</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>86.050003</td>\n",
       "      <td>95.720001</td>\n",
       "      <td>85.269997</td>\n",
       "      <td>94.959999</td>\n",
       "      <td>78.653089</td>\n",
       "      <td>78.336974</td>\n",
       "      <td>77.967901</td>\n",
       "      <td>95.720001</td>\n",
       "      <td>91.548871</td>\n",
       "      <td>2022-06-14</td>\n",
       "      <td>5.498868</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Open       High        Low      Close       EMA6      EMA12  \\\n",
       "In                                                                      \n",
       "425  76.580002  80.529999  75.349998  80.300003  77.562326  77.783698   \n",
       "426  80.540001  82.559998  79.669998  81.379997  78.653089  78.336974   \n",
       "427  81.099998  82.160004  79.239998  79.980003  79.032208  78.589748   \n",
       "428  79.769997  81.599998  76.650002  80.900002  79.565863  78.945172   \n",
       "429  81.019997  82.300003  78.199997  78.559998  79.278473  78.885914   \n",
       "430  78.790001  78.919998  75.820000  76.639999  78.524623  78.540389   \n",
       "431  76.629997  78.190002  75.110001  76.370003  77.909017  78.206483   \n",
       "432  76.459999  77.290001  72.879997  76.320000  77.455012  77.916255   \n",
       "433  75.830002  77.860001  74.779999  77.830002  77.562152  77.902985   \n",
       "434  77.010002  79.629997  76.080002  78.260002  77.761538  77.957911   \n",
       "435  77.900002  79.739998  77.309998  79.110001  78.146813  78.135156   \n",
       "436  79.669998  81.379997  78.769997  81.339996  79.059151  78.628208   \n",
       "437  81.370003  83.580002  80.730003  83.519997  80.333678  79.380791   \n",
       "438  83.500000  87.839996  82.730003  87.680000  82.432627  80.657592   \n",
       "439  88.320000  89.480003  84.830002  86.910004  83.711878  81.619502   \n",
       "440  86.419998  87.449997  84.500000  84.589996  83.962769  82.076501   \n",
       "441  84.589996  90.010002  83.529999  88.690002  85.313407  83.093963   \n",
       "442  88.720001  88.919998  86.360001  88.389999  86.192433  83.908738   \n",
       "443  87.500000  91.050003  86.910004  90.410004  87.397453  84.908932   \n",
       "444  90.790001  94.559998  90.680000  93.550003  89.155325  86.238328   \n",
       "445  93.589996  94.919998  91.000000  93.080002  90.276661  87.290893   \n",
       "446  91.910004  92.620003  88.739998  89.910004  90.171902  87.693833   \n",
       "447  90.169998  90.820000  86.709999  87.180000  89.317073  87.614782   \n",
       "448  85.709999  86.510002  83.050003  84.620003  87.975053  87.154047   \n",
       "449  86.050003  95.720001  85.269997  94.959999  78.653089  78.336974   \n",
       "\n",
       "         EMA24     labels prediction    Datetime    profit trade  \n",
       "In                                                                \n",
       "425  77.671197         nn         nn  2022-01-03  5.498868    15  \n",
       "426  77.967901         nn         nn  2022-01-10  5.498868    15  \n",
       "427  78.128869         nn         nn  2022-01-17  5.498868    15  \n",
       "428  78.350560         nn         nn  2022-01-24  5.498868    15  \n",
       "429  78.367315         nn         nn  2022-01-31  5.498868    15  \n",
       "430  78.229130         nn         nn  2022-02-07  5.498868    15  \n",
       "431  78.080399         nn         nn  2022-02-14  5.498868    15  \n",
       "432  77.939567         nn         nn  2022-02-21  5.498868    15  \n",
       "433  77.930802         nn         nn  2022-02-28  5.498868    15  \n",
       "434  77.957138         nn         nn  2022-03-07  5.498868    15  \n",
       "435  78.049367         nn         nn  2022-03-14  5.498868    15  \n",
       "436  78.312618         nn         nn  2022-03-21  5.498868    15  \n",
       "437  78.729208         nn         nn  2022-03-28  5.498868    15  \n",
       "438  79.445271         nn         nn  2022-04-04  5.498868    15  \n",
       "439  80.042450         nn         nn  2022-04-11  5.498868    15  \n",
       "440  80.406254         nn         nn  2022-04-18  5.498868    15  \n",
       "441  81.068953         nn         nn  2022-04-25  5.498868    15  \n",
       "442  81.654637         nn         nn  2022-05-02  5.498868    15  \n",
       "443  82.355066         nn         nn  2022-05-09  5.498868    15  \n",
       "444  83.250661         nn         nn  2022-05-16  5.498868    15  \n",
       "445  84.037009         nn         nn  2022-05-23  5.498868    15  \n",
       "446  84.506848         nn         nn  2022-05-30  5.498868    15  \n",
       "447  84.720700         nn         nn  2022-06-06  5.498868    15  \n",
       "448  84.712645         nn         nn  2022-06-13  5.498868    15  \n",
       "449  77.967901  95.720001  91.548871  2022-06-14  5.498868    15  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade = 15\n",
    "from plotting import PlotTrade\n",
    "\n",
    "df = PlotTrade(trade=trade,trades_df=trades_df,window_size=window_size,entry_candle=entry_candle,budget=budget)\n",
    "trade += 1\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------> GetPerformanceReport completed\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>EMA6</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>EMA24</th>\n",
       "      <th>labels</th>\n",
       "      <th>prediction</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>profit</th>\n",
       "      <th>trade</th>\n",
       "      <th>Entry</th>\n",
       "      <th>Performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.589996</td>\n",
       "      <td>76.870003</td>\n",
       "      <td>73.120003</td>\n",
       "      <td>73.449997</td>\n",
       "      <td>76.035353</td>\n",
       "      <td>76.249820</td>\n",
       "      <td>75.927199</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2021-09-06</td>\n",
       "      <td>2.650002</td>\n",
       "      <td>1</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73.769997</td>\n",
       "      <td>73.830002</td>\n",
       "      <td>70.889999</td>\n",
       "      <td>71.680000</td>\n",
       "      <td>74.790967</td>\n",
       "      <td>75.546771</td>\n",
       "      <td>75.587423</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2021-09-13</td>\n",
       "      <td>2.650002</td>\n",
       "      <td>1</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71.160004</td>\n",
       "      <td>74.629997</td>\n",
       "      <td>71.120003</td>\n",
       "      <td>73.610001</td>\n",
       "      <td>74.453548</td>\n",
       "      <td>75.248806</td>\n",
       "      <td>75.429230</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2021-09-20</td>\n",
       "      <td>2.650002</td>\n",
       "      <td>1</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73.510002</td>\n",
       "      <td>84.339996</td>\n",
       "      <td>72.660004</td>\n",
       "      <td>81.400002</td>\n",
       "      <td>76.438249</td>\n",
       "      <td>76.195144</td>\n",
       "      <td>75.906891</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2021-09-27</td>\n",
       "      <td>2.650002</td>\n",
       "      <td>1</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84.169998</td>\n",
       "      <td>84.559998</td>\n",
       "      <td>80.150002</td>\n",
       "      <td>80.629997</td>\n",
       "      <td>77.635891</td>\n",
       "      <td>76.877429</td>\n",
       "      <td>76.284740</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2.650002</td>\n",
       "      <td>1</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>85.709999</td>\n",
       "      <td>86.510002</td>\n",
       "      <td>83.050003</td>\n",
       "      <td>84.620003</td>\n",
       "      <td>87.975053</td>\n",
       "      <td>87.154047</td>\n",
       "      <td>84.712645</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-06-13</td>\n",
       "      <td>0.397981</td>\n",
       "      <td>17</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>86.050003</td>\n",
       "      <td>93.790001</td>\n",
       "      <td>85.269997</td>\n",
       "      <td>93.129997</td>\n",
       "      <td>89.447894</td>\n",
       "      <td>88.073424</td>\n",
       "      <td>85.386033</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>0.397981</td>\n",
       "      <td>17</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>93.199997</td>\n",
       "      <td>95.720001</td>\n",
       "      <td>90.389999</td>\n",
       "      <td>92.419998</td>\n",
       "      <td>90.297067</td>\n",
       "      <td>88.742127</td>\n",
       "      <td>85.948750</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-06-27</td>\n",
       "      <td>0.397981</td>\n",
       "      <td>17</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>92.440002</td>\n",
       "      <td>94.510002</td>\n",
       "      <td>90.529999</td>\n",
       "      <td>92.779999</td>\n",
       "      <td>91.006476</td>\n",
       "      <td>89.363338</td>\n",
       "      <td>86.495250</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2022-07-04</td>\n",
       "      <td>0.397981</td>\n",
       "      <td>17</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>92.879997</td>\n",
       "      <td>95.349998</td>\n",
       "      <td>86.019997</td>\n",
       "      <td>87.410004</td>\n",
       "      <td>79.278473</td>\n",
       "      <td>78.885914</td>\n",
       "      <td>78.367315</td>\n",
       "      <td>95.349998</td>\n",
       "      <td>93.277978</td>\n",
       "      <td>2022-07-05</td>\n",
       "      <td>0.397981</td>\n",
       "      <td>17</td>\n",
       "      <td>92.879997</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>425 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Open       High        Low      Close       EMA6      EMA12  \\\n",
       "0    76.589996  76.870003  73.120003  73.449997  76.035353  76.249820   \n",
       "1    73.769997  73.830002  70.889999  71.680000  74.790967  75.546771   \n",
       "2    71.160004  74.629997  71.120003  73.610001  74.453548  75.248806   \n",
       "3    73.510002  84.339996  72.660004  81.400002  76.438249  76.195144   \n",
       "4    84.169998  84.559998  80.150002  80.629997  77.635891  76.877429   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "420  85.709999  86.510002  83.050003  84.620003  87.975053  87.154047   \n",
       "421  86.050003  93.790001  85.269997  93.129997  89.447894  88.073424   \n",
       "422  93.199997  95.720001  90.389999  92.419998  90.297067  88.742127   \n",
       "423  92.440002  94.510002  90.529999  92.779999  91.006476  89.363338   \n",
       "424  92.879997  95.349998  86.019997  87.410004  79.278473  78.885914   \n",
       "\n",
       "         EMA24     labels prediction    Datetime    profit  trade      Entry  \\\n",
       "0    75.927199         nn         nn  2021-09-06  2.650002      1         nn   \n",
       "1    75.587423         nn         nn  2021-09-13  2.650002      1         nn   \n",
       "2    75.429230         nn         nn  2021-09-20  2.650002      1         nn   \n",
       "3    75.906891         nn         nn  2021-09-27  2.650002      1         nn   \n",
       "4    76.284740         nn         nn  2021-10-04  2.650002      1         nn   \n",
       "..         ...        ...        ...         ...       ...    ...        ...   \n",
       "420  84.712645         nn         nn  2022-06-13  0.397981     17         nn   \n",
       "421  85.386033         nn         nn  2022-06-20  0.397981     17         nn   \n",
       "422  85.948750         nn         nn  2022-06-27  0.397981     17         nn   \n",
       "423  86.495250         nn         nn  2022-07-04  0.397981     17         nn   \n",
       "424  78.367315  95.349998  93.277978  2022-07-05  0.397981     17  92.879997   \n",
       "\n",
       "    Performance  \n",
       "0            nn  \n",
       "1            nn  \n",
       "2            nn  \n",
       "3            nn  \n",
       "4            nn  \n",
       "..          ...  \n",
       "420          nn  \n",
       "421          nn  \n",
       "422          nn  \n",
       "423          nn  \n",
       "424        43.0  \n",
       "\n",
       "[425 rows x 14 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from final_evaluation import GetPerformanceReport\n",
    "\n",
    "GetPerformanceReport = GetPerformanceReport()\n",
    "\n",
    "GetPerformanceReport.fit(entry_candle=\"Current Open\",\n",
    "                        budget=10000,\n",
    "                        window_size=25,\n",
    "                        export_excel=True,\n",
    "                        excel_path = excel_reports)\n",
    "\n",
    "trades_df_final = GetPerformanceReport.transform(trades_df)\n",
    "trades_df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Entry candle (Current Close)\n",
      "\n",
      "Budget:  10000\n",
      "\n",
      "Entry price:  91.02\n",
      "Prediction:  93.03\n",
      "Expected Profit:  221.6\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "from final_evaluation import MakeSinglePrediction\n",
    "\n",
    "model_name = f'{saved_models}/{str.upper(ticker)}_{formation_window}_{target_window}_{split_ratio}_{start_date}_{end_date}.h5'\n",
    "\n",
    "MakeSinglePrediction = MakeSinglePrediction()\n",
    "\n",
    "fit_output = MakeSinglePrediction.fit(\n",
    "                        model_name=model_name,\n",
    "                        form_window=24,\n",
    "                        ticker=ticker,\n",
    "                        start_date=\"2021-03-18\",\n",
    "                        end_date=\"2022-08-13\",\n",
    "                        interval='1wk',\n",
    "                        progress=False,\n",
    "                        condition=False,\n",
    "                        timeperiod1=6,\n",
    "                        timeperiod2=12,\n",
    "                        timeperiod3=24,\n",
    "                        debug=False,\n",
    "                        budget=10000,\n",
    "                        penalization=0,\n",
    "                        acceptance=0,\n",
    "                        entry_candle='Current Close')\n",
    "\n",
    "#fit method outputs tuple, get only trade formation out of tuple\n",
    "trade_formation = fit_output[1]\n",
    "\n",
    "###IMPORTANT!!!!\n",
    "#trade_formation dataframe must be checked before transformation, sometimes df pulled via yahoo finance \n",
    "# is shifted, and trade formation does not have entire formation in itself. in this case there must \n",
    "# be changed end_date in fit method\n",
    "\n",
    "MakeSinglePrediction.transform(trade_formation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x575 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotting import PlotCurrentFormation\n",
    "\n",
    "PlotCurrentFormation(trade_formation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>EMA6</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>EMA24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>75.830002</td>\n",
       "      <td>77.860001</td>\n",
       "      <td>74.779999</td>\n",
       "      <td>77.830002</td>\n",
       "      <td>77.562152</td>\n",
       "      <td>77.903023</td>\n",
       "      <td>77.870503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>77.010002</td>\n",
       "      <td>79.629997</td>\n",
       "      <td>76.080002</td>\n",
       "      <td>78.260002</td>\n",
       "      <td>77.761538</td>\n",
       "      <td>77.957942</td>\n",
       "      <td>77.901663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2022-03-14</td>\n",
       "      <td>77.900002</td>\n",
       "      <td>79.739998</td>\n",
       "      <td>77.309998</td>\n",
       "      <td>79.110001</td>\n",
       "      <td>78.146813</td>\n",
       "      <td>78.135182</td>\n",
       "      <td>77.998330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2022-03-21</td>\n",
       "      <td>79.669998</td>\n",
       "      <td>81.379997</td>\n",
       "      <td>78.769997</td>\n",
       "      <td>81.339996</td>\n",
       "      <td>79.059151</td>\n",
       "      <td>78.628231</td>\n",
       "      <td>78.265664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2022-03-28</td>\n",
       "      <td>81.370003</td>\n",
       "      <td>83.580002</td>\n",
       "      <td>80.730003</td>\n",
       "      <td>83.519997</td>\n",
       "      <td>80.333678</td>\n",
       "      <td>79.380810</td>\n",
       "      <td>78.686010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>83.500000</td>\n",
       "      <td>87.839996</td>\n",
       "      <td>82.730003</td>\n",
       "      <td>87.680000</td>\n",
       "      <td>82.432628</td>\n",
       "      <td>80.657608</td>\n",
       "      <td>79.405529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>88.320000</td>\n",
       "      <td>89.480003</td>\n",
       "      <td>84.830002</td>\n",
       "      <td>86.910004</td>\n",
       "      <td>83.711878</td>\n",
       "      <td>81.619515</td>\n",
       "      <td>80.005887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2022-04-18</td>\n",
       "      <td>86.419998</td>\n",
       "      <td>87.449997</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>84.589996</td>\n",
       "      <td>83.962769</td>\n",
       "      <td>82.076512</td>\n",
       "      <td>80.372616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>84.589996</td>\n",
       "      <td>90.010002</td>\n",
       "      <td>83.529999</td>\n",
       "      <td>88.690002</td>\n",
       "      <td>85.313407</td>\n",
       "      <td>83.093972</td>\n",
       "      <td>81.038007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2022-05-02</td>\n",
       "      <td>88.720001</td>\n",
       "      <td>88.919998</td>\n",
       "      <td>86.360001</td>\n",
       "      <td>88.389999</td>\n",
       "      <td>86.192433</td>\n",
       "      <td>83.908746</td>\n",
       "      <td>81.626166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2022-05-09</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>91.050003</td>\n",
       "      <td>86.910004</td>\n",
       "      <td>90.410004</td>\n",
       "      <td>87.397453</td>\n",
       "      <td>84.908939</td>\n",
       "      <td>82.328873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2022-05-16</td>\n",
       "      <td>90.790001</td>\n",
       "      <td>94.559998</td>\n",
       "      <td>90.680000</td>\n",
       "      <td>93.550003</td>\n",
       "      <td>89.155325</td>\n",
       "      <td>86.238334</td>\n",
       "      <td>83.226564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2022-05-23</td>\n",
       "      <td>93.589996</td>\n",
       "      <td>94.919998</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>93.080002</td>\n",
       "      <td>90.276661</td>\n",
       "      <td>87.290898</td>\n",
       "      <td>84.014839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2022-05-30</td>\n",
       "      <td>91.910004</td>\n",
       "      <td>92.620003</td>\n",
       "      <td>88.739998</td>\n",
       "      <td>89.910004</td>\n",
       "      <td>90.171902</td>\n",
       "      <td>87.693837</td>\n",
       "      <td>84.486452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2022-06-06</td>\n",
       "      <td>90.169998</td>\n",
       "      <td>90.820000</td>\n",
       "      <td>86.709999</td>\n",
       "      <td>87.180000</td>\n",
       "      <td>89.317073</td>\n",
       "      <td>87.614786</td>\n",
       "      <td>84.701936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2022-06-13</td>\n",
       "      <td>85.709999</td>\n",
       "      <td>86.510002</td>\n",
       "      <td>83.050003</td>\n",
       "      <td>84.620003</td>\n",
       "      <td>87.975053</td>\n",
       "      <td>87.154050</td>\n",
       "      <td>84.695381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>86.050003</td>\n",
       "      <td>93.790001</td>\n",
       "      <td>85.269997</td>\n",
       "      <td>93.129997</td>\n",
       "      <td>89.447894</td>\n",
       "      <td>88.073426</td>\n",
       "      <td>85.370151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2022-06-27</td>\n",
       "      <td>93.199997</td>\n",
       "      <td>95.720001</td>\n",
       "      <td>90.389999</td>\n",
       "      <td>92.419998</td>\n",
       "      <td>90.297067</td>\n",
       "      <td>88.742130</td>\n",
       "      <td>85.934138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2022-07-04</td>\n",
       "      <td>92.440002</td>\n",
       "      <td>94.510002</td>\n",
       "      <td>90.529999</td>\n",
       "      <td>92.779999</td>\n",
       "      <td>91.006476</td>\n",
       "      <td>89.363340</td>\n",
       "      <td>86.481807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2022-07-11</td>\n",
       "      <td>92.879997</td>\n",
       "      <td>95.349998</td>\n",
       "      <td>92.059998</td>\n",
       "      <td>94.959999</td>\n",
       "      <td>92.136054</td>\n",
       "      <td>90.224365</td>\n",
       "      <td>87.160063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2022-07-18</td>\n",
       "      <td>94.900002</td>\n",
       "      <td>95.099998</td>\n",
       "      <td>88.910004</td>\n",
       "      <td>90.110001</td>\n",
       "      <td>91.557182</td>\n",
       "      <td>90.206770</td>\n",
       "      <td>87.396058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2022-07-25</td>\n",
       "      <td>90.199997</td>\n",
       "      <td>91.949997</td>\n",
       "      <td>87.419998</td>\n",
       "      <td>89.339996</td>\n",
       "      <td>90.923700</td>\n",
       "      <td>90.073420</td>\n",
       "      <td>87.551573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>89.339996</td>\n",
       "      <td>90.019997</td>\n",
       "      <td>86.019997</td>\n",
       "      <td>87.410004</td>\n",
       "      <td>89.919787</td>\n",
       "      <td>89.663664</td>\n",
       "      <td>87.540247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2022-08-08</td>\n",
       "      <td>87.300003</td>\n",
       "      <td>91.029999</td>\n",
       "      <td>86.809998</td>\n",
       "      <td>91.019997</td>\n",
       "      <td>90.234132</td>\n",
       "      <td>89.872331</td>\n",
       "      <td>87.818627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close       EMA6  \\\n",
       "50 2022-02-28  75.830002  77.860001  74.779999  77.830002  77.562152   \n",
       "51 2022-03-07  77.010002  79.629997  76.080002  78.260002  77.761538   \n",
       "52 2022-03-14  77.900002  79.739998  77.309998  79.110001  78.146813   \n",
       "53 2022-03-21  79.669998  81.379997  78.769997  81.339996  79.059151   \n",
       "54 2022-03-28  81.370003  83.580002  80.730003  83.519997  80.333678   \n",
       "55 2022-04-04  83.500000  87.839996  82.730003  87.680000  82.432628   \n",
       "56 2022-04-11  88.320000  89.480003  84.830002  86.910004  83.711878   \n",
       "57 2022-04-18  86.419998  87.449997  84.500000  84.589996  83.962769   \n",
       "58 2022-04-25  84.589996  90.010002  83.529999  88.690002  85.313407   \n",
       "59 2022-05-02  88.720001  88.919998  86.360001  88.389999  86.192433   \n",
       "60 2022-05-09  87.500000  91.050003  86.910004  90.410004  87.397453   \n",
       "61 2022-05-16  90.790001  94.559998  90.680000  93.550003  89.155325   \n",
       "62 2022-05-23  93.589996  94.919998  91.000000  93.080002  90.276661   \n",
       "63 2022-05-30  91.910004  92.620003  88.739998  89.910004  90.171902   \n",
       "64 2022-06-06  90.169998  90.820000  86.709999  87.180000  89.317073   \n",
       "65 2022-06-13  85.709999  86.510002  83.050003  84.620003  87.975053   \n",
       "66 2022-06-20  86.050003  93.790001  85.269997  93.129997  89.447894   \n",
       "67 2022-06-27  93.199997  95.720001  90.389999  92.419998  90.297067   \n",
       "68 2022-07-04  92.440002  94.510002  90.529999  92.779999  91.006476   \n",
       "69 2022-07-11  92.879997  95.349998  92.059998  94.959999  92.136054   \n",
       "70 2022-07-18  94.900002  95.099998  88.910004  90.110001  91.557182   \n",
       "71 2022-07-25  90.199997  91.949997  87.419998  89.339996  90.923700   \n",
       "72 2022-08-01  89.339996  90.019997  86.019997  87.410004  89.919787   \n",
       "73 2022-08-08  87.300003  91.029999  86.809998  91.019997  90.234132   \n",
       "\n",
       "        EMA12      EMA24  \n",
       "50  77.903023  77.870503  \n",
       "51  77.957942  77.901663  \n",
       "52  78.135182  77.998330  \n",
       "53  78.628231  78.265664  \n",
       "54  79.380810  78.686010  \n",
       "55  80.657608  79.405529  \n",
       "56  81.619515  80.005887  \n",
       "57  82.076512  80.372616  \n",
       "58  83.093972  81.038007  \n",
       "59  83.908746  81.626166  \n",
       "60  84.908939  82.328873  \n",
       "61  86.238334  83.226564  \n",
       "62  87.290898  84.014839  \n",
       "63  87.693837  84.486452  \n",
       "64  87.614786  84.701936  \n",
       "65  87.154050  84.695381  \n",
       "66  88.073426  85.370151  \n",
       "67  88.742130  85.934138  \n",
       "68  89.363340  86.481807  \n",
       "69  90.224365  87.160063  \n",
       "70  90.206770  87.396058  \n",
       "71  90.073420  87.551573  \n",
       "72  89.663664  87.540247  \n",
       "73  89.872331  87.818627  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade_formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('04_stockprediction': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ae9b00e5548af57dc5d4c583df0ad518b3d501960f1be5c69ca4a560e00ae05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
